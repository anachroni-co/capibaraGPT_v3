############################
# gpt_oss_120b_finetune.gin
# Template para T5X + SeqIO en TPU v5-64 (mesh 8x8)
# Modelo GPT-OSS-120B - Configuración optimizada para modelo grande
############################

########################
# Entradas generales
########################
TRAINER = @t5x.train.Trainer()

# Directorios - Usar disco local de 1PB
TRAINER.model_dir = "/mnt/1pb-storage/checkpoints/gpt_oss_120b_finetune_model_dir"
TRAINER.init_checkpoint = "/mnt/1pb-storage/models/gpt-oss-120b/checkpoints/base"
TRAINER.checkpoint_save_interval_steps = 1000  # Menos frecuente para modelo grande
TRAINER.checkpoint_keep = 5  # Menos checkpoints para ahorrar espacio

# Checkpointing / TensorStore
TRAINER.checkpointer_class = @t5x.checkpoints.tensorstore_checkpointer.TensorstoreCheckpointer()
TRAINER.checkpointer_class.path = TRAINER.model_dir

########################
# Runtime / TPU / Mesh
########################
# JAX/XLA backend settings
TPU_MESH_SHAPE = (8, 8)  # 8x8 = 64 chips, model_parallel x data_parallel

# dtype
TRAINER.compute_dtype = 'bfloat16'   # precisión recomendada en v5

# pjit/GSPMD partitioning defaults
# Para modelo 120B, necesitamos particionamiento más agresivo
TRAINER.parameter_partition_spec = '2d'   # usar 2D parameter partitioning
TRAINER.activation_partition_spec = '2d'  # 2D activation partitioning

# gradient accumulation para modelo grande
TRAINER.gradient_accumulation_steps = 8  # Más accumulation para 120B

########################
# Modelo GPT-OSS-120B
########################
TRAINER.model_ctor = @models.GPTOSS120BModel
TRAINER.model_ctor.vocab_size = 50257
TRAINER.model_ctor.d_model = 4096        # Modelo más grande
TRAINER.model_ctor.n_layers = 72         # Más capas
TRAINER.model_ctor.n_heads = 64          # Más cabezas de atención
TRAINER.model_ctor.dropout_rate = 0.0
TRAINER.model_ctor.init_scale = 0.01     # Escala más pequeña para modelo grande

########################
# Optimizer & LR schedule
########################
# Adafactor con configuración para modelo grande
TRAINER.optimizer_ctor = @t5x.optimizers.Adafactor
TRAINER.optimizer_ctor.learning_rate = 5e-5  # LR más pequeño para 120B
TRAINER.optimizer_ctor.beta2 = 0.999
TRAINER.optimizer_ctor.weight_decay = 0.01   # Más weight decay
TRAINER.optimizer_ctor.eps = 1e-30
TRAINER.optimizer_ctor.clipping_threshold = 1.0

# Scheduler: warmup más largo para modelo grande
TRAINER.lr_schedule = @t5x.lr_schedules.PolynomialDecaySchedule
TRAINER.lr_schedule.init_value = 5e-5
TRAINER.lr_schedule.end_value = 5e-6
TRAINER.lr_schedule.decay_steps = <TOTAL_TRAINING_STEPS>
TRAINER.lr_schedule.warmup_steps = int(0.02 * <TOTAL_TRAINING_STEPS>)  # 2% warmup

########################
# Pérdida y métricas
########################
TRAINER.loss_fn = @t5x.losses.cross_entropy_loss
TRAINER.loss_fn.label_smoothing = 0.1  # Más label smoothing para modelo grande
TRAINER.eval_metrics_fns = @t5x.metrics.default_metrics_fns

########################
# Datos (SeqIO)
########################
TRAINER.train_task_name = "gpt_oss_120b_finetune:train"
TRAINER.eval_task_name = "gpt_oss_120b_finetune:validation"

# SeqIO: determinismo y sharding
TRAINER.seqio_deterministic = True
TRAINER.seqio_shard_type = "mod"

# Global batch: más pequeño para modelo grande
TRAINER.global_batch_size = 512   # Menos tokens por paso
TRAINER.eval_batch_size = 256

########################
# Data mixture weights (nuevo dataset = 0.8%)
########################
# En el registro de SeqIO:
# seqio.MixtureRegistry.add("gpt_oss_120b_finetune",
#     ["gpt_oss_120b_original", "gpt_oss_120b_new_dataset"],
#     default_rate=lambda name: 0.008 if name == "gpt_oss_120b_new_dataset" else 0.992)

########################
# Logging / eval
########################
TRAINER.log_eval_steps = 2000      # Menos frecuente para modelo grande
TRAINER.eval_interval_steps = 5000
TRAINER.save_eval_checkpoints = True
TRAINER.profile = False

########################
# I/O y rendimiento
########################
TRAINER.prefetch_to_device = 1      # Menos prefetch para ahorrar memoria
TRAINER.tf_data_service = None

########################
# Configuración específica para 120B
########################
# Variables de entorno recomendadas:
# export XLA_FLAGS="--xla_backend=host --xla_tpu_mesh_shape=(8,8) --xla_gspmd_enable=true --xla_gpu_enable_async_all_gather=false"
# export JAX_PLATFORM_NAME=tpu
# export JAX_USE_PJRT_C_API_ON_TPU=1

########################
# FIN del archivo .gin
########################
