"""
Python bindings for ARM NEON optimized kernels
Compatible with NumPy arrays and PyTorch tensors
"""

import ctypes
import numpy as np
from pathlib import Path
from typing import Optional, Union
import os

class NEONKernels:
    """
    Python interface to NEON-optimized C++ kernels

    Usage:
        kernels = NEONKernels()
        C = kernels.matmul_fp32(A, B)  # Fast matrix multiplication
    """

    def __init__(self, lib_path: Optional[str] = None):
        """
        Initialize NEON kernels

        Args:
            lib_path: Path to compiled .so library. If None, auto-detect.
        """
        if lib_path is None:
            lib_path = self._find_library()

        if lib_path and Path(lib_path).exists():
            self.lib = ctypes.CDLL(lib_path)
            self._setup_function_signatures()
            self.available = True
            print(f"âœ… NEON kernels loaded from: {lib_path}")
        else:
            self.available = False
            print("âš ï¸  NEON kernels not compiled. Run: make -C kernels")
            print("   Falling back to NumPy implementations")

    def _find_library(self) -> Optional[str]:
        """Auto-detect compiled library"""
        possible_paths = [
            Path(__file__).parent / "libneon_kernels.so",
            Path(__file__).parent / "build" / "libneon_kernels.so",
            "/usr/local/lib/libneon_kernels.so"
        ]

        for path in possible_paths:
            path_obj = Path(path) if isinstance(path, str) else path
            if path_obj.exists():
                return str(path_obj)

        return None

    def _setup_function_signatures(self):
        """Define C function signatures"""
        # matmul_fp32_neon(A, B, C, M, N, K)
        self.lib.matmul_fp32_neon.argtypes = [
            ctypes.POINTER(ctypes.c_float),  # A
            ctypes.POINTER(ctypes.c_float),  # B
            ctypes.POINTER(ctypes.c_float),  # C
            ctypes.c_int,  # M
            ctypes.c_int,  # N
            ctypes.c_int   # K
        ]
        self.lib.matmul_fp32_neon.restype = None

        # dot_product_fp32_neon(a, b, length) -> float
        self.lib.dot_product_fp32_neon.argtypes = [
            ctypes.POINTER(ctypes.c_float),
            ctypes.POINTER(ctypes.c_float),
            ctypes.c_int
        ]
        self.lib.dot_product_fp32_neon.restype = ctypes.c_float

        # rmsnorm_fp32_neon(input, output, weight, size, eps)
        self.lib.rmsnorm_fp32_neon.argtypes = [
            ctypes.POINTER(ctypes.c_float),
            ctypes.POINTER(ctypes.c_float),
            ctypes.POINTER(ctypes.c_float),
            ctypes.c_int,
            ctypes.c_float
        ]
        self.lib.rmsnorm_fp32_neon.restype = None

        # softmax_fp32_neon(input, output, size)
        self.lib.softmax_fp32_neon.argtypes = [
            ctypes.POINTER(ctypes.c_float),
            ctypes.POINTER(ctypes.c_float),
            ctypes.c_int
        ]
        self.lib.softmax_fp32_neon.restype = None

    def matmul_fp32(
        self,
        A: np.ndarray,
        B: np.ndarray,
        out: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """
        FP32 Matrix Multiplication: C = A @ B

        Args:
            A: [M x K] matrix
            B: [K x N] matrix
            out: Optional output buffer [M x N]

        Returns:
            C: [M x N] result matrix

        Performance:
            ~4x faster than NumPy on ARM Axion
        """
        if not self.available:
            return np.matmul(A, B)

        # Validate inputs
        assert A.dtype == np.float32, "A must be float32"
        assert B.dtype == np.float32, "B must be float32"
        assert A.ndim == 2 and B.ndim == 2, "Inputs must be 2D"
        assert A.shape[1] == B.shape[0], f"Shape mismatch: {A.shape} @ {B.shape}"

        M, K = A.shape
        K2, N = B.shape

        # Allocate output
        if out is None:
            out = np.zeros((M, N), dtype=np.float32)

        # Ensure C-contiguous
        A = np.ascontiguousarray(A)
        B = np.ascontiguousarray(B)

        # Call C function
        self.lib.matmul_fp32_neon(
            A.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            B.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            M, N, K
        )

        return out

    def dot_product(self, a: np.ndarray, b: np.ndarray) -> float:
        """
        Optimized dot product (for attention mechanisms)

        Args:
            a: [N] vector
            b: [N] vector

        Returns:
            Scalar dot product
        """
        if not self.available:
            return np.dot(a, b)

        assert a.dtype == np.float32 and b.dtype == np.float32
        assert a.ndim == 1 and b.ndim == 1
        assert a.shape[0] == b.shape[0]

        a = np.ascontiguousarray(a)
        b = np.ascontiguousarray(b)

        return self.lib.dot_product_fp32_neon(
            a.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            b.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            a.shape[0]
        )

    def rmsnorm(
        self,
        x: np.ndarray,
        weight: np.ndarray,
        eps: float = 1e-6
    ) -> np.ndarray:
        """
        RMSNorm (used in LLaMA, Gemma)

        Args:
            x: [N] input vector
            weight: [N] learned scale parameters
            eps: Small constant for numerical stability

        Returns:
            [N] normalized and scaled output
        """
        if not self.available:
            # NumPy fallback
            rms = np.sqrt(np.mean(x ** 2) + eps)
            return (x / rms) * weight

        assert x.dtype == np.float32 and weight.dtype == np.float32
        assert x.shape == weight.shape

        x = np.ascontiguousarray(x)
        weight = np.ascontiguousarray(weight)
        out = np.zeros_like(x)

        self.lib.rmsnorm_fp32_neon(
            x.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            x.shape[0],
            eps
        )

        return out

    def softmax(self, x: np.ndarray) -> np.ndarray:
        """
        Optimized softmax (for attention)

        Args:
            x: [N] logits

        Returns:
            [N] probabilities (sum to 1)
        """
        if not self.available:
            # NumPy fallback
            exp_x = np.exp(x - np.max(x))
            return exp_x / np.sum(exp_x)

        assert x.dtype == np.float32
        assert x.ndim == 1

        x = np.ascontiguousarray(x)
        out = np.zeros_like(x)

        self.lib.softmax_fp32_neon(
            x.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
            x.shape[0]
        )

        return out

    def check_arm_features(self) -> dict:
        """Check available ARM CPU features"""
        features = {
            'neon': False,
            'fp16': False,
            'dotprod': False,
            'i8mm': False
        }

        try:
            with open('/proc/cpuinfo', 'r') as f:
                cpuinfo = f.read().lower()

            features['neon'] = 'neon' in cpuinfo or 'asimd' in cpuinfo
            features['fp16'] = 'fphp' in cpuinfo or 'asimdhp' in cpuinfo
            features['dotprod'] = 'asimddp' in cpuinfo
            features['i8mm'] = 'i8mm' in cpuinfo
        except:
            pass

        return features

    def get_info(self) -> dict:
        """Get kernel information"""
        return {
            'available': self.available,
            'arm_features': self.check_arm_features(),
            'library_path': getattr(self, 'lib', None),
            'platform': os.uname().machine
        }


# Global instance
_kernels = None

def get_kernels() -> NEONKernels:
    """Get global NEON kernels instance"""
    global _kernels
    if _kernels is None:
        _kernels = NEONKernels()
    return _kernels


# Convenience functions
def matmul(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    """Optimized matrix multiplication"""
    return get_kernels().matmul_fp32(A, B)


def dot(a: np.ndarray, b: np.ndarray) -> float:
    """Optimized dot product"""
    return get_kernels().dot_product(a, b)


def rmsnorm(x: np.ndarray, weight: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    """Optimized RMSNorm"""
    return get_kernels().rmsnorm(x, weight, eps)


def softmax(x: np.ndarray) -> np.ndarray:
    """Optimized softmax"""
    return get_kernels().softmax(x)


if __name__ == '__main__':
    # Test kernels
    print("ğŸ”§ ARM NEON Kernels Test")
    print("=" * 50)

    kernels = get_kernels()
    info = kernels.get_info()

    print(f"Available: {info['available']}")
    print(f"Platform: {info['platform']}")
    print("\nARM Features:")
    for feature, available in info['arm_features'].items():
        status = "âœ…" if available else "âŒ"
        print(f"  {status} {feature.upper()}")

    if kernels.available:
        print("\nğŸš€ Running performance test...")

        # Test matmul
        A = np.random.randn(512, 512).astype(np.float32)
        B = np.random.randn(512, 512).astype(np.float32)

        import time

        # NEON version
        start = time.time()
        C_neon = kernels.matmul_fp32(A, B)
        time_neon = time.time() - start

        # NumPy version
        start = time.time()
        C_numpy = np.matmul(A, B)
        time_numpy = time.time() - start

        print(f"\nâ±ï¸  MatMul 512x512:")
        print(f"   NEON:  {time_neon*1000:.2f}ms")
        print(f"   NumPy: {time_numpy*1000:.2f}ms")
        print(f"   Speedup: {time_numpy/time_neon:.2f}x")

        # Verify correctness
        error = np.max(np.abs(C_neon - C_numpy))
        print(f"   Max error: {error:.2e}")
