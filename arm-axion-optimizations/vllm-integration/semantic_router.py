"""
Semantic Router for vLLM Multi-Expert System
Optimized with ARM NEON for fast routing decisions

Routes requests to appropriate vLLM expert instances based on:
- Semantic analysis
- Domain detection
- Incremental processing (LiveMind style)
"""

import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import numpy as np
import time
import asyncio

sys.path.insert(0, str(Path(__file__).parent.parent))

from kernels.neon_kernels import get_kernels
from vllm_integration.embedding_cache import get_embedding_model, RealEmbeddingModel

@dataclass
class RoutingPrediction:
    """Routing prediction for a request"""
    expert_ids: List[str]
    probabilities: List[float]
    confidence: float
    chunks_processed: int
    can_route: bool


class IncrementalSemanticRouter:
    """
    Router that processes input incrementally and routes to vLLM experts

    Features:
    - NEON-optimized similarity computation
    - Bayesian confidence updates
    - Early routing when confident
    - Compatible with vLLM PagedAttention (shared prefixes)
    """

    def __init__(
        self,
        expert_domains: Dict[str, str],  # expert_id -> domain
        embedding_model: Optional[RealEmbeddingModel] = None,
        use_neon: bool = True,
        chunk_size: int = 64,
        routing_threshold: float = 0.7,
        top_k_experts: int = 2,
        embedding_model_name: str = "all-MiniLM-L6-v2",
        embedding_cache_size: int = 10000
    ):
        """
        Args:
            expert_domains: Mapping of expert_id to domain
            embedding_model: Real embedding model (optional, will create if None)
            use_neon: Enable NEON optimizations
            chunk_size: Tokens per chunk for processing
            routing_threshold: Confidence threshold for routing
            top_k_experts: Number of experts to activate
            embedding_model_name: SentenceTransformer model name
            embedding_cache_size: Max embeddings to cache
        """
        self.expert_domains = expert_domains
        self.num_experts = len(expert_domains)
        self.chunk_size = chunk_size
        self.routing_threshold = routing_threshold
        self.top_k_experts = top_k_experts

        # Real embedding model with cache
        if embedding_model is None:
            print(f"üì• Initializing embedding model: {embedding_model_name}")
            self.embedding_model = get_embedding_model(
                model_name=embedding_model_name,
                cache_size=embedding_cache_size
            )
        else:
            self.embedding_model = embedding_model

        # NEON kernels
        if use_neon:
            self.kernels = get_kernels()
            self.use_neon = self.kernels.available
        else:
            self.kernels = None
            self.use_neon = False

        print(f"‚úÖ Router initialized: {self.num_experts} experts, NEON: {self.use_neon}")
        print(f"   Embedding model: {self.embedding_model.model_name} ({self.embedding_model.embed_dim}d)")
        print(f"   Cache size: {embedding_cache_size}")

        # Expert embeddings (use real embeddings for domains)
        self._initialize_expert_embeddings()

        # Request state tracking
        self.request_states = {}

    def _initialize_expert_embeddings(self):
        """Initialize expert domain embeddings using real embeddings"""
        # Create representative descriptions for each domain
        domain_descriptions = {
            'general': 'general knowledge questions simple answers quick responses casual conversation',
            'technical': 'programming code software development algorithms debugging implementation',
            'multilingual': 'translation multiple languages internacional ÁøªËØë traducci√≥n',
            'expert': 'complex analysis deep reasoning research planning strategic thinking',
            'legal': 'legal law contracts regulations juridical tribunal',
            'finance': 'finance investment capital markets stocks portfolio',
            'medical': 'medical health diagnosis treatment patient symptoms'
        }

        self.expert_embeddings = {}
        expert_ids = list(self.expert_domains.keys())

        # Collect texts to embed in batch
        texts_to_embed = []
        for expert_id in expert_ids:
            domain = self.expert_domains[expert_id]
            description = domain_descriptions.get(domain, domain)
            texts_to_embed.append(description)

        # Batch embed all domains at once (efficient!)
        print(f"üîÑ Computing expert domain embeddings...")
        embeddings = self.embedding_model.embed_batch(texts_to_embed)

        # Store embeddings
        for expert_id, embedding in zip(expert_ids, embeddings):
            self.expert_embeddings[expert_id] = embedding

        print(f"‚úÖ Expert embeddings initialized ({self.embedding_model.embed_dim}d, real embeddings)")

    def start_request(self, request_id: str):
        """Initialize tracking for a new request"""
        self.request_states[request_id] = {
            'expert_probs': np.ones(self.num_experts, dtype=np.float32) / self.num_experts,
            'confidence': 0.0,
            'chunks_processed': 0,
            'embeddings': [],
            'routed': False
        }

    def process_chunk(
        self,
        request_id: str,
        chunk_text: str,
        chunk_embedding: Optional[np.ndarray] = None
    ) -> RoutingPrediction:
        """
        Process a chunk of text and update routing prediction

        Args:
            request_id: Request identifier
            chunk_text: Text chunk
            chunk_embedding: Pre-computed embedding (optional)

        Returns:
            Current routing prediction
        """
        if request_id not in self.request_states:
            self.start_request(request_id)

        state = self.request_states[request_id]

        # Get chunk embedding
        if chunk_embedding is None:
            chunk_embedding = self._embed_chunk(chunk_text)

        # Store embedding
        state['embeddings'].append(chunk_embedding)

        # Compute evidence from this chunk
        chunk_evidence = self._compute_chunk_evidence(chunk_embedding)

        # Bayesian update
        prior = state['expert_probs']
        posterior = prior * chunk_evidence
        posterior = posterior / posterior.sum()  # Normalize

        state['expert_probs'] = posterior
        state['chunks_processed'] += 1

        # Update confidence
        state['confidence'] = self._compute_confidence(posterior, state['chunks_processed'])

        # Can we route?
        can_route = self._can_route(state)

        # Get top experts
        expert_ids = list(self.expert_domains.keys())
        top_indices = np.argsort(posterior)[-self.top_k_experts:][::-1]
        top_expert_ids = [expert_ids[i] for i in top_indices]
        top_probs = [float(posterior[i]) for i in top_indices]

        return RoutingPrediction(
            expert_ids=top_expert_ids,
            probabilities=top_probs,
            confidence=float(state['confidence']),
            chunks_processed=state['chunks_processed'],
            can_route=can_route
        )

    def _embed_chunk(self, chunk_text: str) -> np.ndarray:
        """
        Embed chunk text using real embedding model

        Uses cached embeddings when available (30-40% speedup on repeated texts)
        """
        # Use real embedding model (with automatic caching!)
        embedding = self.embedding_model.embed(chunk_text)
        return embedding

    def _compute_chunk_evidence(self, chunk_embedding: np.ndarray) -> np.ndarray:
        """
        Compute evidence for each expert from chunk embedding

        Uses NEON-optimized dot product if available
        """
        evidence = np.ones(self.num_experts, dtype=np.float32) * 0.1

        expert_ids = list(self.expert_domains.keys())

        for i, expert_id in enumerate(expert_ids):
            expert_emb = self.expert_embeddings[expert_id]

            # Compute cosine similarity
            if self.use_neon and self.kernels:
                # NEON-optimized dot product (5x faster)
                dot = self.kernels.dot_product(
                    chunk_embedding.astype(np.float32),
                    expert_emb.astype(np.float32)
                )
                # Embeddings are already normalized
                similarity = dot
            else:
                # NumPy fallback
                similarity = np.dot(chunk_embedding, expert_emb)

            # Convert similarity [-1, 1] to evidence [0, 2]
            evidence[i] *= (1.0 + similarity)

        return evidence

    def _compute_confidence(
        self,
        probs: np.ndarray,
        chunks_processed: int
    ) -> float:
        """
        Compute routing confidence

        Based on:
        - Entropy of probability distribution
        - Number of chunks processed
        """
        # Entropy-based confidence
        # Low entropy = high confidence
        epsilon = 1e-10
        entropy = -np.sum(probs * np.log(probs + epsilon))
        max_entropy = np.log(self.num_experts)

        # Normalize entropy to [0, 1], then invert
        entropy_confidence = 1.0 - (entropy / max_entropy)

        # Time-based confidence (more chunks = more confident)
        time_confidence = min(chunks_processed / 5.0, 1.0)

        # Combined confidence
        confidence = 0.7 * entropy_confidence + 0.3 * time_confidence

        return confidence

    def _can_route(self, state: Dict[str, Any]) -> bool:
        """
        Determine if we can route with current confidence
        """
        # Already routed
        if state['routed']:
            return True

        # Check confidence threshold
        if state['confidence'] < self.routing_threshold:
            return False

        # Check if there's a dominant expert
        probs = state['expert_probs']
        top_prob = np.max(probs)

        if top_prob > 0.5:  # Clear winner
            return True

        # Or top-k experts have >80% combined probability
        top_k_probs = np.sort(probs)[-self.top_k_experts:]
        if top_k_probs.sum() > 0.8:
            return True

        return False

    def finalize_routing(self, request_id: str) -> RoutingPrediction:
        """
        Finalize routing decision for a request

        Call this when done processing chunks or when ready to route
        """
        if request_id not in self.request_states:
            raise ValueError(f"Unknown request: {request_id}")

        state = self.request_states[request_id]
        state['routed'] = True

        # Get final prediction
        probs = state['expert_probs']
        expert_ids = list(self.expert_domains.keys())

        top_indices = np.argsort(probs)[-self.top_k_experts:][::-1]
        top_expert_ids = [expert_ids[i] for i in top_indices]
        top_probs = [float(probs[i]) for i in top_indices]

        return RoutingPrediction(
            expert_ids=top_expert_ids,
            probabilities=top_probs,
            confidence=float(state['confidence']),
            chunks_processed=state['chunks_processed'],
            can_route=True
        )

    def get_routing_stats(self) -> Dict[str, Any]:
        """Get router statistics including embedding cache performance"""
        embedding_stats = self.embedding_model.get_stats()

        return {
            'num_experts': self.num_experts,
            'active_requests': len(self.request_states),
            'use_neon': self.use_neon,
            'routing_threshold': self.routing_threshold,
            'top_k_experts': self.top_k_experts,
            'embedding_stats': embedding_stats
        }


class FastDomainClassifier:
    """
    Fast keyword-based domain classifier for early routing hints

    Much faster than full embedding, provides quick initial signal
    """

    def __init__(self):
        self.domain_keywords = {
            'legal': [
                'contrato', 'ley', 'art√≠culo', 'jur√≠dico', 'demanda',
                'tribunal', 'sentencia', 'c√≥digo civil', 'derecho',
                'contract', 'law', 'article', 'legal', 'court'
            ],
            'technical': [
                'c√≥digo', 'implementaci√≥n', 'algoritmo', 'funci√≥n', 'clase',
                'variable', 'debug', 'error', 'sistema', 'base de datos',
                'code', 'implementation', 'algorithm', 'function', 'class'
            ],
            'finance': [
                'inversi√≥n', 'rentabilidad', 'capital', 'activo', 'dividendo',
                'mercado', 'acciones', 'fondo', 'riesgo', 'portfolio',
                'investment', 'return', 'capital', 'asset', 'stock'
            ],
            'medical': [
                'diagn√≥stico', 'tratamiento', 's√≠ntoma', 'paciente', 'medicina',
                'enfermedad', 'terapia', 'cl√≠nico', 'doctor', 'hospital',
                'diagnosis', 'treatment', 'symptom', 'patient', 'disease'
            ],
            'general': [
                'informaci√≥n', 'ayuda', 'pregunta', 'explicar', 'c√≥mo',
                'information', 'help', 'question', 'explain', 'how'
            ]
        }

    def classify(self, text: str) -> Tuple[Optional[str], float]:
        """
        Fast classification based on keyword matching

        Args:
            text: Input text

        Returns:
            (domain, confidence) tuple
        """
        text_lower = text.lower()
        scores = {}

        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                scores[domain] = score

        if not scores:
            return None, 0.0

        best_domain = max(scores, key=scores.get)
        max_score = scores[best_domain]

        # Normalize confidence
        total_keywords = len(self.domain_keywords[best_domain])
        confidence = min(max_score / total_keywords, 0.9)

        return best_domain, confidence


if __name__ == '__main__':
    print("üß™ Testing Semantic Router")
    print("=" * 60)

    # Create router
    expert_domains = {
        'expert_legal': 'legal',
        'expert_tech': 'technical',
        'expert_finance': 'finance',
        'expert_medical': 'medical',
        'expert_general': 'general'
    }

    router = IncrementalSemanticRouter(
        expert_domains=expert_domains,
        use_neon=True,
        routing_threshold=0.7,
        top_k_experts=2
    )

    # Test request
    request_id = "test_001"

    test_chunks = [
        "El paciente presenta s√≠ntomas de fiebre alta",
        "y dolor de cabeza persistente desde hace dos d√≠as.",
        "Se recomienda realizar un diagn√≥stico completo",
        "para determinar el tratamiento adecuado."
    ]

    print("\nüìù Processing chunks...")
    for i, chunk in enumerate(test_chunks):
        print(f"\nChunk {i+1}: '{chunk}'")

        prediction = router.process_chunk(request_id, chunk)

        print(f"  Top experts: {prediction.expert_ids}")
        print(f"  Probabilities: {[f'{p:.3f}' for p in prediction.probabilities]}")
        print(f"  Confidence: {prediction.confidence:.3f}")
        print(f"  Can route: {prediction.can_route}")

        if prediction.can_route:
            print(f"  ‚úÖ Ready to route after {prediction.chunks_processed} chunks!")
            break

    # Finalize
    final_prediction = router.finalize_routing(request_id)
    print(f"\n‚úÖ Final routing:")
    print(f"  Experts: {final_prediction.expert_ids}")
    print(f"  Confidence: {final_prediction.confidence:.3f}")

    # Test fast classifier
    print("\n\nüß™ Testing Fast Domain Classifier")
    print("=" * 60)

    classifier = FastDomainClassifier()

    test_texts = [
        "Necesito ayuda con un contrato de arrendamiento",
        "C√≥mo implementar un algoritmo de b√∫squeda binaria en Python",
        "¬øCu√°l es la mejor estrategia de inversi√≥n para un portfolio diversificado?",
        "El paciente tiene s√≠ntomas de gripe y fiebre alta"
    ]

    for text in test_texts:
        domain, confidence = classifier.classify(text)
        print(f"\nText: '{text[:50]}...'")
        print(f"  Domain: {domain}, Confidence: {confidence:.3f}")
