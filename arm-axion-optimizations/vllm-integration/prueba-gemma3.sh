# 1. Verificar archivos del modelo AWQ
cd /home/elect/models/gemma-3-27b-it-awq
ls -lh

# 2. Verificar configuraci√≥n AWQ
cat config.json | grep -E "(quantization|awq)" | head -10

# 3. Test de carga y generaci√≥n con el modelo AWQ
cd ~/capibara6/arm-axion-optimizations/vllm-integration

python3 << 'EOF'
from transformers import AutoModelForCausalLM, AutoProcessor
import torch
import time

print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
print("‚ïë  Test de Gemma 3 27B AWQ INT4 en CPU ARM Axion                    ‚ïë")
print("‚ïë  Comparaci√≥n vs modelo sin quantizar                               ‚ïë")
print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
print()

model_path = "/home/elect/models/gemma-3-27b-it-awq"

print(f"üîÑ Cargando modelo AWQ desde: {model_path}")
print("   Esperado: ~2-3s (similar al anterior)")
start = time.time()

try:
    # Cargar modelo AWQ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="cpu",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    
    load_time = time.time() - start
    print(f"‚úÖ Modelo cargado en {load_time:.1f}s")
    
    # Test de generaci√≥n
    print("\nüß™ Test de generaci√≥n (100 tokens)...")
    prompt = "Explica qu√© es ARM Axion:"
    inputs = processor(text=prompt, return_tensors="pt")
    
    start = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True
    )
    gen_time = time.time() - start
    
    response = processor.decode(outputs[0], skip_special_tokens=True)
    tokens_per_sec = 100 / gen_time
    
    print(f"‚úÖ Generado en {gen_time:.1f}s")
    print(f"\nüìù Respuesta: {response[len(prompt):].strip()[:200]}...")
    
    # Calcular memoria
    import psutil
    process = psutil.Process()
    mem_gb = process.memory_info().rss / 1024**3
    
    print("\nüìä Estad√≠sticas AWQ INT4:")
    print(f"   - Tiempo de carga: {load_time:.1f}s")
    print(f"   - Tiempo de generaci√≥n: {gen_time:.1f}s")
    print(f"   - Velocidad: {tokens_per_sec:.1f} tokens/s")
    print(f"   - Memoria usada: {mem_gb:.1f} GB")
    
    print("\nüìà Comparaci√≥n vs Modelo Original:")
    print(f"   - Velocidad: 0.7 tok/s ‚Üí {tokens_per_sec:.1f} tok/s ({tokens_per_sec/0.7:.1f}x m√°s r√°pido)")
    print(f"   - Memoria: 51.4 GB ‚Üí {mem_gb:.1f} GB ({51.4/mem_gb:.1f}x menos)")
    print(f"   - Tama√±o disco: 52 GB ‚Üí 18 GB (2.9x menos)")
    
    # Evaluar si es aceptable para producci√≥n
    if tokens_per_sec >= 3:
        print("\n‚úÖ Performance ACEPTABLE para producci√≥n")
    elif tokens_per_sec >= 1.5:
        print("\n‚ö†Ô∏è  Performance MARGINAL - considerar optimizaciones adicionales")
    else:
        print("\n‚ùå Performance INSUFICIENTE - necesita m√°s optimizaci√≥n")
    
    print("\n‚úÖ Test exitoso!")
    
except Exception as e:
    print(f"\n‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
    exit(1)
EOF
