# Production Setup - ARM Axion VM

Gu√≠a para deployment en tu VM ARM Axion de Google Cloud con los modelos ya descargados.

## üì¶ Modelos Disponibles

Seg√∫n tu configuraci√≥n actual, tienes estos 4 modelos en `/home/user/models/`:

| Modelo | Tama√±o | Uso | Quantizaci√≥n |
|--------|--------|-----|--------------|
| **phi4-mini-instruct** | ~1.5GB (AWQ) | Respuestas r√°pidas, chat simple | AWQ (4-bit) |
| **mistral-7b-v0.2** | ~3.5GB (AWQ) | C√≥digo, explicaciones t√©cnicas | AWQ (4-bit) |
| **qwen2.5-7b-instruct** | ~4GB (Q4) | Multiling√ºe, an√°lisis de texto | Q4_0 (4-bit) |
| **gpt-oss-20b** | ~10GB (Q4) | Razonamiento complejo, research | Q4_0 (4-bit) |

**Total memoria**: ~19GB ‚Üí Caben todos en tu C4A-standard-32 (128GB RAM)

## üöÄ Quick Start (Producci√≥n)

### 1. Conectar a VM ARM Axion

```bash
# Conectar a tu VM
gcloud compute ssh [NOMBRE_VM_AXION] --zone [ZONA]

# Navegar al directorio
cd ~/capibara6/arm-axion-optimizations/vllm-integration
```

### 2. Verificar Modelos

```bash
# Verificar que los modelos est√©n descargados
ls -lah /home/user/models/

# Deber√≠as ver:
# phi4-mini-instruct/
# mistral-7b-v0.2-instruct/  (o similar)
# qwen2.5-7b-instruct/
# gpt-oss-20b/
```

### 3. Deployment Autom√°tico

```bash
# Ejecutar script de deployment
./deploy-production.sh

# El script:
# ‚úÖ Verifica arquitectura ARM
# ‚úÖ Compila kernels NEON
# ‚úÖ Instala vLLM
# ‚úÖ Configura systemd service
# ‚úÖ Optimiza sistema
```

### 4. Iniciar Servidor

**Opci√≥n A: Manualmente** (para testing)

```bash
python3 inference_server.py --host 0.0.0.0 --port 8080
```

**Opci√≥n B: Systemd** (para producci√≥n)

```bash
# Iniciar
sudo systemctl start vllm-capibara6

# Ver estado
sudo systemctl status vllm-capibara6

# Ver logs en tiempo real
sudo journalctl -u vllm-capibara6 -f

# Habilitar auto-start en boot
sudo systemctl enable vllm-capibara6
```

### 5. Verificar Funcionamiento

```bash
# Health check
curl http://localhost:8080/health

# Listar expertos
curl http://localhost:8080/experts

# Stats
curl http://localhost:8080/stats

# Test completion
curl http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain ARM Axion processors",
    "max_tokens": 100
  }'
```

## üìä Configuraci√≥n de Routing

El sistema autom√°ticamente enruta requests a los expertos apropiados:

```python
# Routing autom√°tico por dominio:

"Hola, ¬øc√≥mo est√°s?"
  ‚Üí phi4_fast (respuesta simple)

"Implementa binary search en Python"
  ‚Üí mistral_balanced (c√≥digo t√©cnico)

"ÁøªËØëËøôÂè•ËØùÂà∞Ëã±ËØ≠"  (Traducir al ingl√©s)
  ‚Üí qwen_multilingual (multiling√ºe)

"Analiza las implicaciones econ√≥micas del cambio clim√°tico"
  ‚Üí gptoss_complex (an√°lisis profundo)
```

## ‚öôÔ∏è Ajustar Configuraci√≥n

### Editar Paths de Modelos

Si tus modelos est√°n en ubicaciones diferentes:

```bash
nano config.production.json
```

Actualizar paths:

```json
{
  "experts": [
    {
      "expert_id": "phi4_fast",
      "model_path": "/ruta/real/a/phi4-mini-instruct",
      ...
    }
  ]
}
```

### Ajustar Memoria

Si tienes menos RAM disponible:

```json
{
  "experts": [
    {
      "gpu_memory_utilization": 0.70,  // Reducir de 0.85
      "max_num_seqs": 128,              // Reducir de 256
      ...
    }
  ]
}
```

### Deshabilitar Expertos

Si quieres usar solo algunos modelos:

```json
{
  "experts": [
    // Comentar o eliminar expertos que no quieras cargar
    {
      "expert_id": "phi4_fast",
      ...
    },
    {
      "expert_id": "mistral_balanced",
      ...
    }
    // qwen y gptoss comentados = no se cargan
  ]
}
```

## üîß Troubleshooting

### Error: "Model not found"

```bash
# Verificar path exacto de modelos
ls -la /home/user/models/

# Actualizar config.production.json con paths correctos
```

### Error: "Out of memory"

```bash
# Opci√≥n 1: Usar menos expertos
# Editar config.production.json y cargar solo 2-3 modelos

# Opci√≥n 2: Reducir batch size
# En config: "max_num_seqs": 64  (en lugar de 256)

# Opci√≥n 3: Usar m√°s agresiva quantizaci√≥n
# Cambiar "awq" ‚Üí "q4_0" para m√°s compression
```

### Performance lento

```bash
# Verificar CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Debe ser "performance"

# Si no:
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Verificar NEON kernels compilados
cd ~/capibara6/arm-axion-optimizations
make info
```

### vLLM no inicia

```bash
# Ver logs detallados
sudo journalctl -u vllm-capibara6 -n 100 --no-pager

# Verificar vLLM instalado
python3 -c "import vllm; print(vllm.__version__)"

# Reinstalar si necesario
pip3 install --upgrade vllm
```

## üìà Monitoring

### Logs

```bash
# Logs del servidor
sudo journalctl -u vllm-capibara6 -f

# Logs de sistema
dmesg | tail -50

# Uso de memoria
watch -n 1 free -h

# Uso de CPU
htop
```

### M√©tricas

```bash
# Stats de vLLM
curl http://localhost:8080/stats | jq

# Info de expertos
curl http://localhost:8080/experts | jq

# Health check continuo
watch -n 5 'curl -s http://localhost:8080/health | jq'
```

## üîÑ Updates

### Actualizar c√≥digo

```bash
cd ~/capibara6
git pull origin main

# Recompilar kernels
cd arm-axion-optimizations
make clean && make all

# Reiniciar servicio
sudo systemctl restart vllm-capibara6
```

### Actualizar vLLM

```bash
pip3 install --upgrade vllm

# Reiniciar
sudo systemctl restart vllm-capibara6
```

## üéØ Casos de Uso Optimizados

### RAG con Multi-Expert

```python
import openai

openai.api_base = "http://[TU-VM-IP]:8080/v1"
openai.api_key = "dummy"

# El router autom√°ticamente selecciona el mejor experto
response = openai.ChatCompletion.create(
    model="default",
    messages=[
        {"role": "system", "content": "Eres un asistente experto"},
        {"role": "user", "content": "Analiza este c√≥digo Python: ..."}
    ]
)
# ‚Üí Usa mistral_balanced (experto en c√≥digo)
```

### High-Throughput

```python
# M√∫ltiples requests en paralelo
# vLLM continuous batching los procesa eficientemente

import asyncio

async def generate_many():
    tasks = [
        generate_async(prompt)
        for prompt in prompts_list
    ]
    return await asyncio.gather(*tasks)

# Throughput esperado: 150-200 req/min
```

## üìû Soporte

- **Issues**: https://github.com/anacronic-io/capibara6/issues
- **Email**: marco@anachroni.co
- **Docs**: arm-axion-optimizations/vllm-integration/README.md

---

**Optimizado para**: Google Cloud C4A (ARM Axion)
**√öltima actualizaci√≥n**: 2025-11-19
