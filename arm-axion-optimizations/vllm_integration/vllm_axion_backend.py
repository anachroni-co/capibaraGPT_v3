"""
vLLM ARM Axion Backend
Custom backend para vLLM optimizado para ARM Axion processors

Integraci√≥n con vLLM existente, agregando:
- NEON-optimized kernels
- Q4/Q8 quantization support
- ARM-specific memory optimizations
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
import numpy as np

# Add parent directory to path for our kernels
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from kernels.neon_kernels import get_kernels
    from quantization.quantize import get_quantizer
    NEON_AVAILABLE = True
except ImportError:
    NEON_AVAILABLE = False
    print("‚ö†Ô∏è  NEON kernels not available, using vLLM defaults")

# vLLM imports
try:
    from vllm import LLM, SamplingParams
    from vllm.config import ModelConfig, ParallelConfig, SchedulerConfig
    from vllm.engine.arg_utils import EngineArgs, AsyncEngineArgs
    from vllm.engine.llm_engine import LLMEngine
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    print("‚ö†Ô∏è  vLLM not installed. Install: pip install vllm")


class AxionVLLMConfig:
    """
    Configuraci√≥n optimizada para vLLM en ARM Axion
    """

    def __init__(
        self,
        model_path: str,
        quantization: Optional[str] = None,  # 'awq', 'gptq', 'q4_0', 'q8_0'
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.90,
        max_num_seqs: int = 256,
        max_model_len: Optional[int] = None,
        enable_neon: bool = True,
        enable_chunked_prefill: bool = True,
        max_num_batched_tokens: Optional[int] = None,
    ):
        """
        Args:
            model_path: Path to model weights
            quantization: Quantization method
            tensor_parallel_size: Number of tensor parallel GPUs
            gpu_memory_utilization: GPU memory utilization (0-1)
            max_num_seqs: Max number of sequences in batch
            max_model_len: Max model sequence length
            enable_neon: Enable NEON optimizations
            enable_chunked_prefill: Enable chunked prefill for lower TTFT
            max_num_batched_tokens: Max tokens in batch (for prefill)
        """
        self.model_path = model_path
        self.quantization = quantization
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_num_seqs = max_num_seqs
        self.max_model_len = max_model_len
        self.enable_neon = enable_neon and NEON_AVAILABLE
        self.enable_chunked_prefill = enable_chunked_prefill
        self.max_num_batched_tokens = max_num_batched_tokens or (
            8192 if enable_chunked_prefill else None
        )

    def to_engine_args(self) -> Dict[str, Any]:
        """Convert to vLLM EngineArgs format"""
        args = {
            'model': self.model_path,
            'tensor_parallel_size': self.tensor_parallel_size,
            'gpu_memory_utilization': self.gpu_memory_utilization,
            'max_num_seqs': self.max_num_seqs,
            'trust_remote_code': True,
        }

        if self.quantization:
            # vLLM native quantization
            if self.quantization in ['awq', 'gptq', 'squeezellm']:
                args['quantization'] = self.quantization
            # Our custom Q4/Q8 (requires custom backend)
            elif self.quantization in ['q4_0', 'q8_0']:
                args['quantization'] = None  # Load as FP16, quantize ourselves
                args['load_format'] = 'auto'

        if self.max_model_len:
            args['max_model_len'] = self.max_model_len

        if self.enable_chunked_prefill:
            args['enable_chunked_prefill'] = True
            args['max_num_batched_tokens'] = self.max_num_batched_tokens

        return args


class AxionVLLMEngine:
    """
    vLLM Engine wrapper optimized for ARM Axion

    Provides:
    - NEON-accelerated operations where possible
    - Custom Q4/Q8 quantization
    - ARM-optimized memory management
    - Integration with semantic routing
    """

    def __init__(
        self,
        config: AxionVLLMConfig,
        engine_id: Optional[str] = None
    ):
        """
        Initialize vLLM engine with Axion optimizations

        Args:
            config: Axion-optimized configuration
            engine_id: Unique identifier for this engine (for multi-expert)
        """
        if not VLLM_AVAILABLE:
            raise ImportError("vLLM not installed. Install: pip install vllm")

        self.config = config
        self.engine_id = engine_id or "default"

        # Initialize NEON kernels if available
        if config.enable_neon and NEON_AVAILABLE:
            self.kernels = get_kernels()
            self.use_neon = self.kernels.available
            print(f"‚úÖ [{self.engine_id}] NEON kernels enabled")
        else:
            self.kernels = None
            self.use_neon = False

        # Initialize quantizer if needed
        if config.quantization in ['q4_0', 'q8_0']:
            self.quantizer = get_quantizer()
            print(f"‚úÖ [{self.engine_id}] Custom quantization: {config.quantization}")
        else:
            self.quantizer = None

        # Create vLLM engine (sync for batch operations)
        print(f"üöÄ [{self.engine_id}] Initializing vLLM engine...")
        engine_args = config.to_engine_args()

        try:
            self.llm = LLM(**engine_args)
            print(f"‚úÖ [{self.engine_id}] vLLM engine ready")
        except Exception as e:
            print(f"‚ùå [{self.engine_id}] Failed to initialize vLLM: {e}")
            raise

        # Create async engine for streaming (lazy initialization)
        self.async_engine = None
        self.async_engine_args = engine_args

        # Stats
        self.total_requests = 0
        self.total_tokens_generated = 0

    def generate(
        self,
        prompts: List[str],
        sampling_params: Optional[SamplingParams] = None,
        use_tqdm: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Generate completions for prompts

        Args:
            prompts: List of prompt strings
            sampling_params: vLLM sampling parameters
            use_tqdm: Show progress bar

        Returns:
            List of generation results
        """
        if sampling_params is None:
            sampling_params = SamplingParams(
                temperature=0.7,
                top_p=0.9,
                max_tokens=512,
            )

        # Generate with vLLM
        outputs = self.llm.generate(
            prompts,
            sampling_params,
            use_tqdm=use_tqdm
        )

        # Process outputs
        results = []
        for output in outputs:
            result = {
                'prompt': output.prompt,
                'text': output.outputs[0].text,
                'tokens': output.outputs[0].token_ids,
                'finish_reason': output.outputs[0].finish_reason,
                'engine_id': self.engine_id
            }
            results.append(result)

            self.total_tokens_generated += len(output.outputs[0].token_ids)

        self.total_requests += len(prompts)

        return results

    async def _get_async_engine(self):
        """
        Get or create async engine for streaming (lazy initialization)

        Returns:
            AsyncLLMEngine instance
        """
        if self.async_engine is None:
            import asyncio

            print(f"üöÄ [{self.engine_id}] Initializing async engine for streaming...")

            # Create AsyncEngineArgs from engine_args
            async_args = AsyncEngineArgs(**self.async_engine_args)

            # Create async engine
            self.async_engine = AsyncLLMEngine.from_engine_args(async_args)

            print(f"‚úÖ [{self.engine_id}] Async engine ready")

        return self.async_engine

    async def generate_streaming(
        self,
        prompt: str,
        sampling_params: Optional[SamplingParams] = None,
        request_id: Optional[str] = None
    ):
        """
        Generate completion with TRUE streaming (async generator)

        Uses vLLM's AsyncLLMEngine for real token-by-token streaming
        with minimal latency (TTFT ~0.05-0.1s vs simulated ~0.2s+)

        Args:
            prompt: Single prompt string
            sampling_params: vLLM sampling parameters
            request_id: Optional request ID for tracking

        Yields:
            Token strings as they're generated in real-time
        """
        import uuid

        if sampling_params is None:
            sampling_params = SamplingParams(
                temperature=0.7,
                top_p=0.9,
                max_tokens=512,
            )

        if request_id is None:
            request_id = str(uuid.uuid4())

        # Get async engine (lazy init)
        engine = await self._get_async_engine()

        # Add request to async engine
        results_generator = engine.generate(
            prompt=prompt,
            sampling_params=sampling_params,
            request_id=request_id
        )

        # Stream tokens as they're generated
        previous_text = ""
        async for request_output in results_generator:
            # Get the new text (delta from previous)
            current_text = request_output.outputs[0].text
            new_text = current_text[len(previous_text):]
            previous_text = current_text

            if new_text:
                yield new_text

            # Track tokens generated
            if request_output.finished:
                self.total_tokens_generated += len(request_output.outputs[0].token_ids)
                self.total_requests += 1

    def get_stats(self) -> Dict[str, Any]:
        """Get engine statistics"""
        return {
            'engine_id': self.engine_id,
            'total_requests': self.total_requests,
            'total_tokens_generated': self.total_tokens_generated,
            'neon_enabled': self.use_neon,
            'quantization': self.config.quantization,
            'model': self.config.model_path
        }


class AxionMultiExpertVLLM:
    """
    Multi-expert system using multiple vLLM instances with lazy loading

    Each expert is a separate vLLM engine, allowing:
    - Different models per expert
    - Different quantization per expert
    - Parallel generation across experts
    - Lazy loading: Only load experts when needed (saves memory)
    - LRU eviction: Unload least-used experts when memory is tight
    - Shared system prompt via PagedAttention (vLLM handles this)
    """

    def __init__(
        self,
        expert_configs: List[Dict[str, Any]],
        use_lazy_loading: bool = True,
        warmup_pool_size: int = 2,
        max_loaded_experts: int = 4,
        memory_threshold: float = 0.80
    ):
        """
        Initialize multi-expert system

        Args:
            expert_configs: List of configs, each with:
                - model_path: Path to expert model
                - quantization: Quantization method
                - domain: Expert domain (e.g., 'legal', 'technical')
                - priority: Loading priority (higher = more important)
                - ... (other AxionVLLMConfig params)
            use_lazy_loading: Enable lazy loading (recommended for >2 experts)
            warmup_pool_size: Number of experts to preload (highest priority)
            max_loaded_experts: Maximum experts loaded simultaneously
            memory_threshold: Unload LRU when memory > this (0-1)
        """
        self.expert_configs = expert_configs
        self.use_lazy_loading = use_lazy_loading

        # Store expert metadata (domains, configs, etc.)
        self.experts = {}
        for i, expert_config in enumerate(expert_configs):
            expert_id = expert_config.get('expert_id', f"expert_{i}")
            domain = expert_config.get('domain', 'general')

            self.experts[expert_id] = {
                'domain': domain,
                'config_dict': expert_config
            }

        if use_lazy_loading:
            # Import lazy manager here to avoid circular dependency
            from vllm_integration.lazy_expert_manager import LazyExpertManager

            print(f"üöÄ Initializing lazy loading for {len(expert_configs)} experts...")
            print(f"   Warmup pool: {warmup_pool_size}")
            print(f"   Max loaded: {max_loaded_experts}")

            self.lazy_manager = LazyExpertManager(
                expert_configs=expert_configs,
                warmup_pool_size=warmup_pool_size,
                max_loaded_experts=max_loaded_experts,
                memory_threshold=memory_threshold,
                enable_auto_unload=True
            )

            print(f"‚úÖ Lazy loading initialized")
        else:
            # Old behavior: Load all experts at once
            self.lazy_manager = None
            print(f"üöÄ Loading all {len(expert_configs)} experts (no lazy loading)...")

            for i, expert_config in enumerate(expert_configs):
                expert_id = expert_config.get('expert_id', f"expert_{i}")
                domain = expert_config.get('domain', 'general')

                # Create config
                config = AxionVLLMConfig(
                    model_path=expert_config['model_path'],
                    quantization=expert_config.get('quantization', None),
                    tensor_parallel_size=expert_config.get('tensor_parallel_size', 1),
                    gpu_memory_utilization=expert_config.get('gpu_memory_utilization', 0.85),
                    max_num_seqs=expert_config.get('max_num_seqs', 128),
                    enable_neon=expert_config.get('enable_neon', True),
                    enable_chunked_prefill=expert_config.get('enable_chunked_prefill', True),
                )

                # Create engine
                engine = AxionVLLMEngine(config, engine_id=expert_id)

                self.experts[expert_id]['engine'] = engine
                self.experts[expert_id]['config'] = config

                print(f"‚úÖ Expert '{expert_id}' ({domain}) ready")

            print(f"‚úÖ All {len(self.experts)} experts loaded")

    async def get_expert(
        self,
        expert_id: str,
        predicted_probability: float = 1.0
    ) -> Optional[AxionVLLMEngine]:
        """
        Get expert by ID (lazy loads if necessary)

        Args:
            expert_id: Expert identifier
            predicted_probability: Routing probability (for eviction decisions)

        Returns:
            Expert engine or None
        """
        if expert_id not in self.experts:
            return None

        if self.use_lazy_loading:
            # Use lazy manager to get (and possibly load) expert
            return await self.lazy_manager.get_expert(expert_id, predicted_probability)
        else:
            # Direct access (all experts already loaded)
            expert_info = self.experts.get(expert_id)
            return expert_info.get('engine') if expert_info else None

    def get_expert_sync(self, expert_id: str) -> Optional[AxionVLLMEngine]:
        """
        Get expert by ID (synchronous version for non-lazy mode)

        Only works when lazy loading is disabled
        """
        if self.use_lazy_loading:
            raise RuntimeError("Use async get_expert() with lazy loading enabled")

        expert_info = self.experts.get(expert_id)
        return expert_info.get('engine') if expert_info else None

    async def get_expert_by_domain(self, domain: str) -> Optional[AxionVLLMEngine]:
        """Get expert by domain (lazy loads if necessary)"""
        for expert_id, expert_info in self.experts.items():
            if expert_info['domain'] == domain:
                return await self.get_expert(expert_id)
        return None

    def list_experts(self) -> List[Dict[str, Any]]:
        """List all experts with their info"""
        if self.use_lazy_loading:
            # Get stats from lazy manager
            manager_stats = self.lazy_manager.get_stats()
            return manager_stats['experts']
        else:
            # Old behavior: get stats from loaded engines
            return [
                {
                    'expert_id': expert_id,
                    'domain': info['domain'],
                    'model': info['config'].model_path,
                    'quantization': info['config'].quantization,
                    'is_loaded': True,
                    'stats': info['engine'].get_stats() if 'engine' in info else {}
                }
                for expert_id, info in self.experts.items()
            ]

    async def generate_parallel(
        self,
        prompt: str,
        expert_ids: List[str],
        expert_probabilities: Optional[List[float]] = None,
        sampling_params: Optional[SamplingParams] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate from multiple experts in parallel

        Args:
            prompt: Prompt string (same for all experts)
            expert_ids: List of expert IDs to use
            expert_probabilities: Routing probabilities (for lazy loading priority)
            sampling_params: Sampling parameters

        Returns:
            List of results from each expert
        """
        import asyncio

        if expert_probabilities is None:
            expert_probabilities = [1.0] * len(expert_ids)

        tasks = []
        for expert_id, prob in zip(expert_ids, expert_probabilities):
            # Get expert (will lazy load if necessary)
            expert = await self.get_expert(expert_id, predicted_probability=prob)

            if expert:
                # Note: In real implementation, use vLLM's async API
                task = asyncio.create_task(
                    asyncio.to_thread(
                        expert.generate,
                        [prompt],
                        sampling_params
                    )
                )
                tasks.append((expert_id, task))

        # Wait for all experts
        results = []
        for expert_id, task in tasks:
            expert_result = await task
            results.append({
                'expert_id': expert_id,
                'result': expert_result[0]
            })

        return results

    async def generate_streaming_from_expert(
        self,
        expert_id: str,
        prompt: str,
        sampling_params: Optional[SamplingParams] = None,
        expert_probability: float = 1.0,
        request_id: Optional[str] = None
    ):
        """
        Stream generation from specific expert

        Args:
            expert_id: Expert to use
            prompt: Prompt string
            sampling_params: Sampling parameters
            expert_probability: Routing probability (for lazy loading)
            request_id: Optional request ID

        Yields:
            Token strings as they're generated
        """
        # Get expert (lazy loads if necessary)
        expert = await self.get_expert(expert_id, predicted_probability=expert_probability)

        if not expert:
            raise ValueError(f"Expert not found: {expert_id}")

        # Stream from expert
        async for token in expert.generate_streaming(prompt, sampling_params, request_id):
            yield token

    def get_manager_stats(self) -> Dict[str, Any]:
        """Get lazy manager statistics"""
        if self.use_lazy_loading:
            return self.lazy_manager.get_stats()
        else:
            return {
                'lazy_loading_enabled': False,
                'total_experts': len(self.experts),
                'loaded_experts': len(self.experts)
            }


def create_axion_vllm_config_from_capibara(
    model_config: Dict[str, Any]
) -> AxionVLLMConfig:
    """
    Create AxionVLLMConfig from Capibara6 model config

    Args:
        model_config: Config from vm-bounty2/config/models_config.py

    Returns:
        AxionVLLMConfig ready for engine creation
    """
    # Extract relevant fields
    model_path = model_config.get('model_path', model_config.get('base_model'))
    quantization = model_config.get('quantization', None)

    # Map Capibara config to vLLM config
    config = AxionVLLMConfig(
        model_path=model_path,
        quantization=quantization,
        tensor_parallel_size=model_config.get('tensor_parallel_size', 1),
        gpu_memory_utilization=model_config.get('gpu_memory_utilization', 0.90),
        max_num_seqs=model_config.get('max_num_seqs', 256),
        max_model_len=model_config.get('max_model_len', None),
        enable_neon=model_config.get('optimizations', {}).get('neon', True),
        enable_chunked_prefill=True,
    )

    return config


if __name__ == '__main__':
    print("üß™ Testing Axion vLLM Backend")
    print("=" * 60)

    if not VLLM_AVAILABLE:
        print("‚ùå vLLM not installed. Install: pip install vllm")
        sys.exit(1)

    # Example: Single expert
    print("\nüìù Test 1: Single Expert")
    config = AxionVLLMConfig(
        model_path="facebook/opt-125m",  # Small model for testing
        quantization=None,
        enable_neon=True,
        enable_chunked_prefill=True,
        max_num_seqs=4
    )

    try:
        engine = AxionVLLMEngine(config, engine_id="test_expert")

        # Generate
        results = engine.generate(
            ["Hello, how are you?"],
            sampling_params=SamplingParams(temperature=0.8, max_tokens=50)
        )

        print(f"\n‚úÖ Generated: {results[0]['text']}")
        print(f"\nüìä Stats: {engine.get_stats()}")

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
