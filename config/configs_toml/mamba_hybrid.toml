# Configuraci贸n H铆brida Mamba-Transformer para Capibara-6
# Optimizada para TPU v6e-64 con routing inteligente

[model]
name = "Capibara-6-Mamba-Hybrid"
version = "3.0.0"
hidden_size = 768
num_layers = 24
num_heads = 12
vocab_size = 50257
max_position_embeddings = 2048
dtype = "bfloat16"
param_dtype = "float32"

[modules]
# M贸dulos activos con Mamba y Hybrid Attention
active = [
    "core_transformer",
    "mamba",              #  M贸dulo Mamba O(n)
    "hybrid_attention",   #  Atenci贸n h铆brida inteligente
    "attention_module", 
    "feedforward_module",
    "embedding_module"
]

# Configuraci贸n del m贸dulo Mamba
[modules.mamba]
enabled = true
hidden_size = 768
d_state = 64           # Dimensi贸n del estado SSM
d_conv = 4             # Kernel size convoluci贸n 1D
expand_factor = 2      # Factor expansi贸n proyecciones
dt_rank = 32          # Rango par谩metro temporal
use_bias = true
use_conv_bias = true
activation = "swish"   # swish, gelu, relu
layer_norm_epsilon = 1e-5

# Optimizaciones Mamba TPU
use_tpu_optimizations = true
use_mixed_precision = true
scan_type = "associative"  # "linear", "associative"

# Configuraci贸n del m贸dulo h铆brido
[modules.hybrid_attention]
enabled = true
hidden_size = 768
num_heads = 12
intermediate_size = 3072

# Par谩metros de decisi贸n h铆brida
mamba_threshold = 512        # Longitud m铆nima para Mamba
transformer_max_length = 2048  # Longitud m谩xima Transformer

# Configuraci贸n Mamba dentro del h铆brido
[modules.hybrid_attention.mamba_config]
hidden_size = 768
d_state = 64
d_conv = 4
expand_factor = 2
use_tpu_optimizations = true

# Configuraci贸n Transformer dentro del h铆brido
dropout_rate = 0.1
layer_norm_eps = 1e-12

# Optimizaciones h铆bridas
use_tpu_optimizations = true
use_mixed_precision = true
enable_caching = true
collect_metrics = true
log_decisions = false  # Cambiar a true para debugging

# Configuraci贸n m贸dulos existentes
[modules.core_transformer]
enabled = true
layers = 24
hidden_size = 768

[modules.attention_module]
enabled = true
num_heads = 12
head_dim = 64
dropout = 0.1

[modules.feedforward_module]
enabled = true
intermediate_size = 3072
activation = "gelu"
dropout = 0.1

[modules.embedding_module]
enabled = true
vocab_size = 50257
hidden_size = 768
max_position_embeddings = 2048

[training]
batch_size = 32
learning_rate = 1e-4
weight_decay = 0.01
warmup_steps = 1000
max_steps = 100000
gradient_clip_norm = 1.0

# Configuraci贸n espec铆fica para entrenamiento h铆brido
enable_adaptive_batching = true  # Ajustar batch seg煤n m贸dulo activo
mamba_batch_multiplier = 1.5     # Batch m谩s grande para Mamba (m谩s eficiente)
transformer_batch_multiplier = 1.0

[optimization]
optimizer = "adamw"
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Optimizaciones espec铆ficas para Mamba
mamba_lr_multiplier = 1.2        # LR ligeramente mayor para Mamba
hybrid_gradient_scaling = true   # Escalado diferencial de gradientes

[tpu]
# TPU v6e-64 configuration optimizada para Mamba
enabled = true
mesh_shape = [8, 8]  # dp=8, ep=8
use_bf16 = true
enable_flash_attention = true

# Optimizaciones Mamba-espec铆ficas TPU
enable_mamba_scan_optimization = true
associative_scan_threshold = 512
tpu_memory_optimization = true

[moe]
# MoE system configuration (compatible con Mamba)
enabled = true
config_file = "moe_system.toml"
mamba_moe_integration = true  # Integrar Mamba en sistema MoE

[rag]
# RAG system configuration 
enabled = true
config_file = "advanced_rag.toml"

[hybrid_routing]
# Configuraci贸n avanzada de routing h铆brido
enabled = true

# Reglas de decisi贸n
[hybrid_routing.decision_rules]
short_sequence_max = 256      # Siempre Transformer
medium_sequence_max = 1024    # H铆brido seg煤n carga
long_sequence_min = 1024      # Siempre Mamba
very_long_sequence_min = 2048 # Forzar Mamba

# Factores de decisi贸n
memory_pressure_threshold = 0.8  # Cambiar a Mamba si memoria >80%
training_mode_preference = "mamba"  # Preferir Mamba en training
inference_mode_preference = "auto"  # Auto-decidir en inference

# M茅tricas y monitoreo
[hybrid_routing.metrics]
collect_decision_stats = true
collect_performance_metrics = true
log_routing_decisions = false  # Cambiar a true para debugging
decision_history_size = 1000

# Configuraci贸n de fallbacks
[hybrid_routing.fallbacks]
mamba_fallback_to_transformer = true
transformer_fallback_to_mamba = false  # Transformer no fallback a Mamba
error_fallback_module = "transformer"   # Usar Transformer si hay errores

[logging]
level = "INFO"
format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Logging espec铆fico para Mamba
[logging.mamba]
level = "INFO"
log_ssm_operations = false    # Cambiar a true para debugging detallado
log_scan_operations = false   # Cambiar a true para debugging scan
log_routing_decisions = false # Cambiar a true para debugging routing

[paths]
model_dir = "models"
data_dir = "data"
cache_dir = "cache"
logs_dir = "logs"

# Paths espec铆ficos para Mamba
mamba_cache_dir = "cache/mamba"
hybrid_metrics_dir = "logs/hybrid_metrics"