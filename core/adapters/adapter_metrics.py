"""
Adapter Metrics System

Sistema autom√°tico of metrics para monitorear y optimizar
el rendimiento de todos los adapters del system.
"""

import logging
import time
import threading
from typing import Any, Dict, List, Optional, Callable
from enum import Enum
from dataclasses import dataclass, field
from collections import deque, defaultdict
import statistics
import json
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class MetricType(Enum):
    """Tipos of metrics monitoreadas."""
    EXECUTION_TIME = "execution_time"
    SUCCESS_RATE = "success_rate"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"
    MEMORY_USAGE = "memory_usage"
    CACHE_HIT_RATE = "cache_hit_rate"
    ADAPTER_UTILIZATION = "adapter_utilization"
    PERFORMANCE_SCORE = "performance_score"

class AlertLevel(Enum):
    """Niveles de alerta para m√©tricas."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class MetricThreshold:
    """Umbral de m√©trica para alertas."""
    metric_type: MetricType
    adapter_name: str
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    alert_level: AlertLevel = AlertLevel.WARNING
    enabled: bool = True

@dataclass
class MetricEvent:
    """Recorded metric event."""
    timestamp: float
    adapter_name: str
    metric_type: MetricType
    value: float
    operation: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Alert:
    """Alert generated by threshold violation."""
    timestamp: float
    adapter_name: str
    metric_type: MetricType
    threshold: MetricThreshold
    current_value: float
    alert_level: AlertLevel
    message: str
    acknowledged: bool = False

class PerformanceTracker:
    """Tracker de rendimiento para adapters individuales."""
    
    def __init__(self, adapter_name: str, history_size: int = 1000):
        self.adapter_name = adapter_name
        self.history_size = history_size
        self.metrics_history: Dict[MetricType, deque] = {
            metric_type: deque(maxlen=history_size) for metric_type in MetricType
        }
        self.operation_metrics: Dict[str, Dict[MetricType, deque]] = defaultdict(
            lambda: {metric_type: deque(maxlen=100) for metric_type in MetricType}
        )
        self.start_time = time.time()
        self.lock = threading.RLock()
        
    def record_metric(self, metric_type: MetricType, value: float, operation: str = ""):
        """Records a metric."""
        with self.lock:
            timestamp = time.time()
            
            # Registrar en historial general
            self.metrics_history[metric_type].append((timestamp, value))
            
            # Registrar por operaci√≥n si se especifica
            if operation:
                self.operation_metrics[operation][metric_type].append((timestamp, value))
    
    def get_current_metrics(self) -> Dict[MetricType, float]:
        """Gets current metrics (average of recent measurements)."""
        current_metrics = {}
        
        with self.lock:
            for metric_type in MetricType:
                history = self.metrics_history[metric_type]
                if history:
                    # Average of the last 5 measurements
                    recent_values = [value for _, value in list(history)[-5:]]
                    current_metrics[metric_type] = statistics.mean(recent_values)
                else:
                    current_metrics[metric_type] = 0.0
        
        return current_metrics
    
    def get_metric_statistics(self, metric_type: MetricType, window_minutes: int = 60) -> Dict[str, float]:
        """Gets metric statistics in a time window."""
        with self.lock:
            cutoff_time = time.time() - (window_minutes * 60)
            history = self.metrics_history[metric_type]
            
            # Filtrar por ventana de tiempo
            recent_values = [value for timestamp, value in history if timestamp >= cutoff_time]
            
            if not recent_values:
                return {'count': 0, 'mean': 0.0, 'median': 0.0, 'min': 0.0, 'max': 0.0, 'std': 0.0}
            
            return {
                'count': len(recent_values),
                'mean': statistics.mean(recent_values),
                'median': statistics.median(recent_values),
                'min': min(recent_values),
                'max': max(recent_values),
                'std': statistics.stdev(recent_values) if len(recent_values) > 1 else 0.0
            }
    
    def get_trend(self, metric_type: MetricType, window_size: int = 10) -> float:
        """Calculates metric trend (slope)."""
        with self.lock:
            history = list(self.metrics_history[metric_type])
            
            if len(history) < 2:
                return 0.0
            
            recent_history = history[-window_size:]
            if len(recent_history) < 2:
                return 0.0
            
            # Regresi√≥n lineal simple
            n = len(recent_history)
            sum_x = sum(i for i in range(n))
            sum_y = sum(value for _, value in recent_history)
            sum_xy = sum(i * value for i, (_, value) in enumerate(recent_history))
            sum_x2 = sum(i * i for i in range(n))
            
            denominator = n * sum_x2 - sum_x * sum_x
            if denominator == 0:
                return 0.0
            
            slope = (n * sum_xy - sum_x * sum_y) / denominator
            return slope
    
    def get_operation_performance(self, operation: str) -> Dict[str, Any]:
        """Gets operation-specific performance."""
        with self.lock:
            if operation not in self.operation_metrics:
                return {}
            
            op_metrics = {}
            for metric_type, history in self.operation_metrics[operation].items():
                if history:
                    values = [value for _, value in history]
                    op_metrics[metric_type.value] = {
                        'count': len(values),
                        'mean': statistics.mean(values),
                        'latest': values[-1] if values else 0.0
                    }
                else:
                    op_metrics[metric_type.value] = {'count': 0, 'mean': 0.0, 'latest': 0.0}
            
            return op_metrics
    
    def calculate_performance_score(self) -> float:
        """Calculates an overall performance score."""
        current_metrics = self.get_current_metrics()
        
        # Factores de score (pesos)
        weights = {
            MetricType.EXECUTION_TIME: -0.3,  # Negativo porque menor es mejor
            MetricType.SUCCESS_RATE: 0.4,     # Positivo porque mayor es mejor
            MetricType.THROUGHPUT: 0.2,       # Positivo porque mayor es mejor
            MetricType.ERROR_RATE: -0.3,      # Negativo porque menor es mejor
            MetricType.CACHE_HIT_RATE: 0.1    # Positivo porque mayor es mejor
        }
        
        score = 0.0
        total_weight = 0.0
        
        for metric_type, weight in weights.items():
            value = current_metrics.get(metric_type, 0.0)
            
            # Normalizar valores seg√∫n el tipo de m√©trica
            if metric_type == MetricType.EXECUTION_TIME:
                # Normalizar tiempo de ejecuci√≥n (menor es mejor)
                normalized_value = max(0, 1.0 - (value / 1000.0))  # 1s = score 0
            elif metric_type in [MetricType.SUCCESS_RATE, MetricType.CACHE_HIT_RATE]:
                # Ya est√°n normalizados (0-1)
                normalized_value = value
            elif metric_type == MetricType.THROUGHPUT:
                # Normalizar throughput (mayor es mejor)
                normalized_value = min(value / 100.0, 1.0)  # 100 ops/s = score 1
            elif metric_type == MetricType.ERROR_RATE:
                # Normalizar error rate (menor es mejor)
                normalized_value = max(0, 1.0 - value)
            else:
                normalized_value = value
            
            score += weight * normalized_value
            total_weight += abs(weight)
        
        # Normalizar score final
        if total_weight > 0:
            score = score / total_weight
        
        # Asegurar que est√© en rango [0, 1]
        return max(0.0, min(1.0, score + 0.5))  # +0.5 para centrar en 0.5

class AdapterMetricsCollector:
    """Colector central of metrics para todos los adapters."""
    
    def __init__(self):
        self.trackers: Dict[str, PerformanceTracker] = {}
        self.thresholds: List[MetricThreshold] = []
        self.alerts: deque = deque(maxlen=1000)
        self.alert_callbacks: List[Callable[[Alert], None]] = []
        self.collection_active = False
        self.collection_thread = None
        self.collection_interval = 30.0  # segundos
        self.lock = threading.RLock()
        
        # Configurar umbrales por defecto
        self._setup_default_thresholds()
        
    def register_adapter(self, adapter_name: str) -> PerformanceTracker:
        """Registers an adapter for monitoring."""
        with self.lock:
            if adapter_name not in self.trackers:
                tracker = PerformanceTracker(adapter_name)
                self.trackers[adapter_name] = tracker
                logger.info(f"Registered adapter for metrics: {adapter_name}")
            
            return self.trackers[adapter_name]
    
    def record_execution(self, 
                        adapter_name: str, 
                        operation: str,
                        execution_time_ms: float,
                        success: bool,
                        metadata: Optional[Dict[str, Any]] = None):
        """Records an adapter execution."""
        tracker = self.register_adapter(adapter_name)
        
        # Registrar m√©tricas b√°sicas
        tracker.record_metric(MetricType.EXECUTION_TIME, execution_time_ms, operation)
        tracker.record_metric(MetricType.SUCCESS_RATE, 1.0 if success else 0.0, operation)
        tracker.record_metric(MetricType.ERROR_RATE, 0.0 if success else 1.0, operation)
        
        # Calcular throughput (operaciones por segundo)
        current_time = time.time()
        if hasattr(tracker, '_last_throughput_calculation'):
            time_diff = current_time - tracker._last_throughput_calculation
            if time_diff >= 1.0:  # Calcular cada segundo
                ops_count = len([t for t, _ in tracker.metrics_history[MetricType.EXECUTION_TIME] 
                               if t >= current_time - 1.0])
                tracker.record_metric(MetricType.THROUGHPUT, ops_count, operation)
                tracker._last_throughput_calculation = current_time
        else:
            tracker._last_throughput_calculation = current_time
        
        # Verificar umbrales
        self._check_thresholds(adapter_name, tracker)
    
    def start_collection(self):
        """Starts automatic collection of metrics."""
        if self.collection_active:
            return
        
        self.collection_active = True
        self.collection_thread = threading.Thread(target=self._collection_loop, daemon=True)
        self.collection_thread.start()
        logger.info("Metrics collection started")
    
    def stop_collection(self):
        """Stops collection of metrics."""
        self.collection_active = False
        if self.collection_thread:
            self.collection_thread.join(timeout=1.0)
        logger.info("Metrics collection stopped")
    
    def get_adapter_metrics(self, adapter_name: str) -> Dict[str, Any]:
        """Gets complete adapter metrics."""
        with self.lock:
            if adapter_name not in self.trackers:
                return {}
            
            tracker = self.trackers[adapter_name]
            current_metrics = tracker.get_current_metrics()
            
            # Statistics detalladas
            detailed_stats = {}
            for metric_type in MetricType:
                detailed_stats[metric_type.value] = tracker.get_metric_statistics(metric_type)
            
            # Tendencias
            trends = {}
            for metric_type in MetricType:
                trends[metric_type.value] = tracker.get_trend(metric_type)
            
            return {
                'adapter_name': adapter_name,
                'uptime_seconds': time.time() - tracker.start_time,
                'current_metrics': {k.value: v for k, v in current_metrics.items()},
                'detailed_statistics': detailed_stats,
                'trends': trends,
                'performance_score': tracker.calculate_performance_score(),
                'total_operations': sum(len(history) for history in tracker.metrics_history.values())
            }
    
    def get_system_overview(self) -> Dict[str, Any]:
        """Gets complete metrics system overview."""
        with self.lock:
            overview = {
                'total_adapters': len(self.trackers),
                'collection_active': self.collection_active,
                'total_alerts': len(self.alerts),
                'unacknowledged_alerts': len([a for a in self.alerts if not a.acknowledged]),
                'adapters_summary': {},
                'system_performance': {}
            }
            
            # Resumen por adapter
            total_ops = 0
            total_score = 0.0
            
            for adapter_name, tracker in self.trackers.items():
                current_metrics = tracker.get_current_metrics()
                performance_score = tracker.calculate_performance_score()
                
                ops_count = sum(len(history) for history in tracker.metrics_history.values())
                total_ops += ops_count
                total_score += performance_score
                
                overview['adapters_summary'][adapter_name] = {
                    'performance_score': performance_score,
                    'total_operations': ops_count,
                    'avg_execution_time': current_metrics.get(MetricType.EXECUTION_TIME, 0.0),
                    'success_rate': current_metrics.get(MetricType.SUCCESS_RATE, 0.0),
                    'status': self._get_adapter_status(current_metrics)
                }
            
            # M√©tricas del system
            overview['system_performance'] = {
                'total_operations': total_ops,
                'average_system_score': total_score / len(self.trackers) if self.trackers else 0.0,
                'collection_uptime': time.time() - getattr(self, '_collection_start_time', time.time())
            }
            
            return overview
    
    def get_alerts(self, limit: int = 50, unacknowledged_only: bool = False) -> List[Dict[str, Any]]:
        """Gets system alerts."""
        with self.lock:
            alerts = list(self.alerts)
            
            if unacknowledged_only:
                alerts = [a for a in alerts if not a.acknowledged]
            
            # Ordenar por timestamp (m√°s recientes primero)
            alerts.sort(key=lambda x: x.timestamp, reverse=True)
            
            # Limitar resultados
            alerts = alerts[:limit]
            
            return [
                {
                    'timestamp': a.timestamp,
                    'datetime': datetime.fromtimestamp(a.timestamp).isoformat(),
                    'adapter_name': a.adapter_name,
                    'metric_type': a.metric_type.value,
                    'alert_level': a.alert_level.value,
                    'current_value': a.current_value,
                    'threshold_min': a.threshold.min_value,
                    'threshold_max': a.threshold.max_value,
                    'message': a.message,
                    'acknowledged': a.acknowledged
                } for a in alerts
            ]
    
    def acknowledge_alert(self, alert_index: int) -> bool:
        """Marca una alerta como reconocida."""
        with self.lock:
            try:
                if 0 <= alert_index < len(self.alerts):
                    self.alerts[alert_index].acknowledged = True
                    return True
                return False
            except IndexError:
                return False
    
    def add_threshold(self, threshold: MetricThreshold):
        """A√±ade un umbral de m√©trica."""
        with self.lock:
            self.thresholds.append(threshold)
            logger.info(f"Added threshold for {threshold.adapter_name}.{threshold.metric_type.value}")
    
    def add_alert_callback(self, callback: Callable[[Alert], None]):
        """A√±ade callback para alertas."""
        self.alert_callbacks.append(callback)
    
    def export_metrics(self, format: str = "json") -> str:
        """Exporta m√©tricas en formato especificado."""
        overview = self.get_system_overview()
        
        if format.lower() == "json":
            return json.dumps(overview, indent=2, default=str)
        else:
            raise ValueError(f"Unsupported export format: {format}")
    
    def _setup_default_thresholds(self):
        """Configura umbrales por defecto."""
        default_thresholds = [
            # Tiempo de ejecuci√≥n
            MetricThreshold(MetricType.EXECUTION_TIME, "*", max_value=5000.0, alert_level=AlertLevel.WARNING),
            MetricThreshold(MetricType.EXECUTION_TIME, "*", max_value=10000.0, alert_level=AlertLevel.ERROR),
            
            # Tasa de √©xito
            MetricThreshold(MetricType.SUCCESS_RATE, "*", min_value=0.95, alert_level=AlertLevel.WARNING),
            MetricThreshold(MetricType.SUCCESS_RATE, "*", min_value=0.90, alert_level=AlertLevel.ERROR),
            
            # Tasa de error
            MetricThreshold(MetricType.ERROR_RATE, "*", max_value=0.05, alert_level=AlertLevel.WARNING),
            MetricThreshold(MetricType.ERROR_RATE, "*", max_value=0.10, alert_level=AlertLevel.ERROR),
            
            # Performance score
            MetricThreshold(MetricType.PERFORMANCE_SCORE, "*", min_value=0.7, alert_level=AlertLevel.WARNING),
            MetricThreshold(MetricType.PERFORMANCE_SCORE, "*", min_value=0.5, alert_level=AlertLevel.ERROR)
        ]
        
        self.thresholds.extend(default_thresholds)
    
    def _collection_loop(self):
        """Main collection loop of metrics."""
        self._collection_start_time = time.time()
        
        while self.collection_active:
            try:
                # Recolectar m√©tricas del system
                self._collect_system_metrics()
                
                # Calcular performance scores
                self._update_performance_scores()
                
                time.sleep(self.collection_interval)
                
            except Exception as e:
                logger.warning(f"Error in metrics collection loop: {e}")
                time.sleep(5.0)
    
    def _collect_system_metrics(self):
        """Collects system metrics."""
        try:
            # M√©tricas de memoria del proceso
            memory_usage = self._get_process_memory_usage()
            
            # Registrar para todos los adapters activos
            for adapter_name, tracker in self.trackers.items():
                tracker.record_metric(MetricType.MEMORY_USAGE, memory_usage)
        
        except Exception as e:
            logger.debug(f"Error collecting system metrics: {e}")
    
    def _update_performance_scores(self):
        """Updates performance scores."""
        with self.lock:
            for adapter_name, tracker in self.trackers.items():
                try:
                    score = tracker.calculate_performance_score()
                    tracker.record_metric(MetricType.PERFORMANCE_SCORE, score)
                except Exception as e:
                    logger.debug(f"Error calculating performance score for {adapter_name}: {e}")
    
    def _check_thresholds(self, adapter_name: str, tracker: PerformanceTracker):
        """Checks thresholds and generates alerts."""
        current_metrics = tracker.get_current_metrics()
        
        for threshold in self.thresholds:
            if not threshold.enabled:
                continue
            
            # Verificar si el umbral aplica a este adapter
            if threshold.adapter_name != "*" and threshold.adapter_name != adapter_name:
                continue
            
            current_value = current_metrics.get(threshold.metric_type, 0.0)
            violated = False
            
            # Verificar violaci√≥n de umbral
            if threshold.min_value is not None and current_value < threshold.min_value:
                violated = True
            elif threshold.max_value is not None and current_value > threshold.max_value:
                violated = True
            
            if violated:
                alert = Alert(
                    timestamp=time.time(),
                    adapter_name=adapter_name,
                    metric_type=threshold.metric_type,
                    threshold=threshold,
                    current_value=current_value,
                    alert_level=threshold.alert_level,
                    message=f"{adapter_name} {threshold.metric_type.value} = {current_value:.3f} violates threshold"
                )
                
                self._generate_alert(alert)
    
    def _generate_alert(self, alert: Alert):
        """Generates an alert."""
        with self.lock:
            self.alerts.append(alert)
            
            # Llamar callbacks
            for callback in self.alert_callbacks:
                try:
                    callback(alert)
                except Exception as e:
                    logger.warning(f"Alert callback failed: {e}")
            
            logger.warning(f"ALERT [{alert.alert_level.value.upper()}]: {alert.message}")
    
    def _get_process_memory_usage(self) -> float:
        """Gets process memory usage."""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)  # MB
        except ImportError:
            return 0.0
        except Exception:
            return 0.0
    
    def _get_adapter_status(self, current_metrics: Dict[MetricType, float]) -> str:
        """Determines the state de un adapter based en m√©tricas."""
        success_rate = current_metrics.get(MetricType.SUCCESS_RATE, 1.0)
        error_rate = current_metrics.get(MetricType.ERROR_RATE, 0.0)
        performance_score = current_metrics.get(MetricType.PERFORMANCE_SCORE, 0.5)
        
        if success_rate >= 0.95 and error_rate <= 0.05 and performance_score >= 0.7:
            return "healthy"
        elif success_rate >= 0.90 and error_rate <= 0.10 and performance_score >= 0.5:
            return "warning"
        else:
            return "critical"

# Global metrics collector instance
metrics_collector = AdapterMetricsCollector()

# Decorator for automatic monitoring
def monitor_adapter_performance(adapter_name: str, operation: str = ""):
    """Decorador para monitorear autom√°ticamente el rendimiento de adapters."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            success = True
            error = None
            
            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                success = False
                error = e
                raise
            finally:
                execution_time = (time.time() - start_time) * 1000  # ms
                
                # Registrar m√©tricas
                metrics_collector.record_execution(
                    adapter_name=adapter_name,
                    operation=operation or func.__name__,
                    execution_time_ms=execution_time,
                    success=success,
                    metadata={'error': str(error) if error else None}
                )
        
        return wrapper
    return decorator

# Funciones de utilidad
def start_metrics_collection():
    """Starts automatic collection of metrics."""
    metrics_collector.start_collection()

def stop_metrics_collection():
    """Stops collection of metrics."""
    metrics_collector.stop_collection()

def get_metrics_overview():
    """Gets metrics system overview."""
    return metrics_collector.get_system_overview()

def get_adapter_performance(adapter_name: str):
    """Gets performance metrics for a specific adapter."""
    return metrics_collector.get_adapter_metrics(adapter_name)

def export_metrics_report(format: str = "json"):
    """Exporta reporte of metrics."""
    return metrics_collector.export_metrics(format)

# Example callback for alerts
def default_alert_callback(alert: Alert):
    """Callback por defecto para alertas."""
    level_emoji = {
        AlertLevel.INFO: "‚ÑπÔ∏è",
        AlertLevel.WARNING: "‚ö†Ô∏è", 
        AlertLevel.ERROR: "‚ùå",
        AlertLevel.CRITICAL: "üö®"
    }
    
    emoji = level_emoji.get(alert.alert_level, "‚ùì")
    print(f"{emoji} ALERT: {alert.message}")

# Registrar callback por defecto
metrics_collector.add_alert_callback(default_alert_callback)