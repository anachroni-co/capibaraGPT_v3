# Activations Module

M√≥dulo para funciones de activaci√≥n contextual optimizadas para arquitecturas neuronales avanzadas.

## üìã Descripci√≥n

Este m√≥dulo proporciona funciones de activaci√≥n contextual que adaptan su comportamiento basado en el contexto de entrada, optimizando el rendimiento del modelo en diferentes tipos de tareas.

## üîß Componentes

### ContextualActivation (`contextual_activation.py`)
Sistema base para activaciones contextualmente conscientes.

```python
from capibara.core.activations import contextual_activation

# Configuraci√≥n b√°sica del m√≥dulo
logger = contextual_activation.logger
result = contextual_activation.main()

# Integraci√≥n con JAX/Flax
import jax
import flax.linen as nn
from capibara.core.activations.contextual_activation import *
```

## üöÄ Caracter√≠sticas

### Activaciones Adaptativas
- **Contexto-Aware**: Las activaciones se adaptan seg√∫n el contexto de entrada
- **JAX/Flax Integration**: Optimizado para TPU v4/v6 usando JAX y Flax
- **Logging Avanzado**: Sistema de logging integrado para monitoreo

### Optimizaciones Hardware
- **TPU Ready**: Preparado para TPU v4-32 y v6e-64
- **Memory Efficient**: Gesti√≥n eficiente de memoria
- **Vectorizaci√≥n**: Soporte para operaciones vectorizadas

## üéØ Casos de Uso

### 1. Activaciones Contextualmente Conscientes
```python
# Ejemplo de uso b√°sico
from capibara.core.activations import contextual_activation
import jax.numpy as jnp

# Inicializar m√≥dulo
result = contextual_activation.main()

# Usar con modelo Flax
class ContextualModel(nn.Module):
    def __call__(self, x):
        # Aplicar activaci√≥n contextual
        return contextual_activation.apply(x)
```

### 2. Integraci√≥n con Pipelines
```python
# Integraci√≥n en pipelines de procesamiento
from capibara.core.activations.contextual_activation import logger

# Logging de activaciones
logger.info("Aplicando activaciones contextuales")

# Procesamiento en lotes
def process_batch(inputs):
    logger.info(f"Procesando lote de tama√±o: {len(inputs)}")
    # Aplicar activaciones contextuales
    return processed_outputs
```

## üèóÔ∏è Arquitectura

```
activations/
‚îú‚îÄ‚îÄ __init__.py              # Exports del m√≥dulo
‚îú‚îÄ‚îÄ contextual_activation.py # Sistema base de activaciones
‚îî‚îÄ‚îÄ README.md               # Documentaci√≥n
```

## ‚öôÔ∏è Configuraci√≥n

### Par√°metros de Activaci√≥n
```python
# Configuraci√≥n de activaci√≥n contextual
activation_config = {
    "context_window": 512,
    "adaptation_rate": 0.1,
    "temperature": 0.8,
    "enable_caching": True
}
```

### Variables de Entorno
```bash
# Configuraciones de sistema
export JAX_PLATFORMS=tpu
export CAPIBARA_ACTIVATION_LOG_LEVEL=INFO
export CAPIBARA_CONTEXT_CACHE_SIZE=1024
```

## üîç Funciones de Activaci√≥n Disponibles

### Activaciones B√°sicas
- **ContextualReLU**: ReLU adaptativo basado en contexto
- **ContextualGELU**: GELU con par√°metros contextuales
- **ContextualSiLU**: SiLU (Swish) contextualmente aware

### Activaciones Avanzadas
- **AdaptiveActivation**: Combina m√∫ltiples funciones basado en contexto
- **MetaActivation**: Aprende la funci√≥n de activaci√≥n √≥ptima
- **HierarchicalActivation**: Activaciones jer√°rquicas por capas

## üìä Monitoreo y M√©tricas

### M√©tricas de Rendimiento
```python
# M√©tricas de activaci√≥n
metrics = {
    "activation_distribution": "Normal",
    "gradient_flow": "Stable",
    "saturation_rate": 0.05,
    "context_adaptation": 0.92
}
```

### Logging Estructurado
```python
import logging
from capibara.core.activations.contextual_activation import logger

# Configurar logging
logger.setLevel(logging.INFO)

# M√©tricas detalladas
logger.info("Activaci√≥n contextual iniciada")
logger.debug(f"Par√°metros de contexto: {context_params}")
```

## üöÄ Optimizaciones de Rendimiento

### TPU Optimizations
- **XLA Compilation**: Compilaci√≥n autom√°tica para TPU
- **Memory Layout**: Distribuci√≥n √≥ptima de memoria
- **Batch Processing**: Procesamiento eficiente en lotes

### T√©cnicas Avanzadas
- **Gradient Checkpointing**: Reducci√≥n de uso de memoria
- **Mixed Precision**: Soporte para bfloat16
- **Kernel Fusion**: Fusi√≥n de operaciones para mayor eficiencia

## üîß Desarrollo y Extensi√≥n

### Crear Nueva Activaci√≥n
```python
from capibara.core.activations.contextual_activation import logger
import jax.numpy as jnp
import flax.linen as nn

class CustomContextualActivation(nn.Module):
    context_dim: int = 768
    
    def setup(self):
        self.context_projection = nn.Dense(self.context_dim)
        
    def __call__(self, x, context=None):
        if context is not None:
            context_features = self.context_projection(context)
            # Aplicar activaci√≥n basada en contexto
            return jnp.tanh(x * context_features)
        return jnp.tanh(x)
```

### Testing y Validaci√≥n
```python
# Tests unitarios para activaciones
def test_contextual_activation():
    from capibara.core.activations import contextual_activation
    
    result = contextual_activation.main()
    assert result == True
    
    # Verificar integraci√≥n JAX
    assert contextual_activation.jax is not None
    assert contextual_activation.jnp is not None
```

## üìö Referencias

- [JAX Documentation](https://jax.readthedocs.io/)
- [Flax Neural Networks](https://flax.readthedocs.io/)
- [TPU Programming Guide](https://cloud.google.com/tpu/docs/)
- [Contextual Activations Research](https://arxiv.org/abs/...)

## ü§ù Contribuir

Para contribuir al m√≥dulo de activaciones:

1. Implementar nuevas funciones de activaci√≥n en `contextual_activation.py`
2. Agregar tests unitarios
3. Documentar par√°metros y comportamiento
4. Optimizar para TPU cuando sea posible
5. Seguir las convenciones de c√≥digo del proyecto