#!/usr/bin/env python3
"""
Prueba local del sistema de router sem√°ntico y consenso
"""

import sys
import os
import json
import asyncio
import aiohttp
from typing import Dict, Any, List

# A√±adir las carpetas al path
sys.path.insert(0, '/home/elect/capibara6/vm-bounty2')
sys.path.insert(0, '/home/elect/capibara6/vm-bounty2/config')

def test_local_model_config():
    """Probar la configuraci√≥n local de modelos"""
    print("üîç Verificando configuraci√≥n de modelos...")
    from config.models_config import get_system_info, get_model_config, get_active_models, get_prompt_template
    
    info = get_system_info()
    print(f"   ‚úÖ Modelos activos: {info['active_models']}/{info['total_models']}")
    print(f"   üìã Modelos: {info['models_list']}")
    print(f"   ü§ù Consenso habilitado: {info['consensus_enabled']}")
    
    # Probar acceso a configuraciones espec√≠ficas
    for model_id in info['models_list']:
        config = get_model_config(model_id)
        print(f"   üß† {model_id}: {config['name']} ({config['type']})")
    
    return True

def test_prompt_templates():
    """Probar las plantillas de prompts"""
    print("\nüìù Verificando plantillas de prompts...")
    from config.models_config import get_prompt_template, get_available_templates, get_models_for_template
    
    templates = get_available_templates()
    print(f"   üéØ Plantillas disponibles: {templates}")
    
    for template_id in templates:
        template = get_prompt_template(template_id)
        models = get_models_for_template(template_id)
        print(f"   üìù {template_id}: {template['description']} -> {models}")
    
    return True

def test_consenso_logic():
    """Probar la l√≥gica de consenso"""
    print("\nü§ù Verificando l√≥gica de consenso...")
    from config.models_config import CONSENSUS_CONFIG
    
    print(f"   üéØ M√©todo de votaci√≥n: {CONSENSUS_CONFIG['voting_method']}")
    print(f"   üìä M√≠n. modelos: {CONSENSUS_CONFIG['min_models']}")
    print(f"   üìà M√°x. modelos: {CONSENSUS_CONFIG['max_models']}")
    print(f"   üîÑ Modelo fallback: {CONSENSUS_CONFIG['fallback_model']}")
    print(f"   ‚öñÔ∏è  Pesos: {CONSENSUS_CONFIG['model_weights']}")
    
    return True

def test_format_prompt():
    """Probar la funci√≥n de formateo de prompts"""
    print("\nüí¨ Verificando formateo de prompts...")
    from config.models_config import format_prompt
    
    test_prompt = "Hola, ¬øc√≥mo est√°s?"
    
    # Probar con diferentes modelos
    for model_id in ['phi4', 'qwen2.5-coder', 'gpt-oss-20b']:
        formatted = format_prompt(model_id, 'general', test_prompt)
        print(f"   ü§ñ {model_id}: {len(formatted)} caracteres")
    
    return True

def test_model_routing_logic():
    """Probar la l√≥gica de enrutamiento basada en palabras clave (simulada)"""
    print("\nüß≠ Verificando l√≥gica de enrutamiento...")
    
    # Simulaci√≥n de l√≥gica de enrutamiento basada en palabras clave
    def classify_task(prompt: str) -> str:
        """Clasificaci√≥n simple basada en palabras clave (similar a task_classifier.py)"""
        prompt_lower = prompt.lower()
        
        # Palabras clave para tareas complejas
        complex_keywords = ['an√°lisis', 'razonamiento', 'comparaci√≥n', 'evaluar', 'estrategia', 'planificaci√≥n', 'investigaci√≥n', 'profundo', 'detalle', 'complejo', 't√©cnico']
        
        # Palabras clave para tareas intermedias
        balanced_keywords = ['explicar', 'qu√© es', 'c√≥mo funciona', 'describir', 'resumen', 'breve', 'ejemplo', 'definir', 'c√≥digo', 'programaci√≥n']
        
        # Palabras clave para tareas simples
        simple_keywords = ['qu√©', 'qui√©n', 'cu√°l', 'cu√°ndo', 'd√≥nde', 'chiste', 'broma', 'saludo', 'ayuda']
        
        complex_score = sum(1 for keyword in complex_keywords if keyword in prompt_lower)
        balanced_score = sum(1 for keyword in balanced_keywords if keyword in prompt_lower)
        simple_score = sum(1 for keyword in simple_keywords if keyword in prompt_lower)
        
        # Tambi√©n considerar la longitud del prompt
        if len(prompt) > 200:
            complex_score += 1
        elif len(prompt) > 100:
            balanced_score += 1
            
        scores = {
            'complex': complex_score,
            'balanced': balanced_score,
            'simple': simple_score
        }
        
        # Escoger el modelo con mayor puntuaci√≥n
        chosen_task = max(scores, key=scores.get)
        
        print(f"   üìù Prompt: '{prompt[:30]}{'...' if len(prompt) > 30 else ''}'")
        print(f"   üìä Puntuaciones - simple: {scores['simple']}, balanced: {scores['balanced']}, complex: {scores['complex']}")
        print(f"   üéØ Clasificaci√≥n: {chosen_task}")
        
        # Mapear a modelos reales
        if chosen_task == 'complex':
            return 'gpt-oss-20b'
        elif chosen_task == 'balanced':
            return 'qwen2.5-coder'  # o 'mixtral' dependiendo del contenido
        else:
            return 'phi4'  # modelo r√°pido para tareas simples
    
    # Pruebas de enrutamiento
    test_queries = [
        "¬øQu√© es Python?",
        "Escribe un c√≥digo en Python para calcular la serie de Fibonacci",
        "Analiza las implicaciones √©ticas de la inteligencia artificial en la sociedad moderna",
        "Cuentame un chiste",
        "Explica c√≥mo funciona un transformer en inteligencia artificial"
    ]
    
    for query in test_queries:
        selected_model = classify_task(query)
        print(f"   üß† Ruta elegida: {selected_model}")
        print()
    
    return True

async def test_consensus_simulation():
    """Simular el proceso de consenso entre modelos"""
    print("ü§ù Simulando proceso de consenso...")
    
    from config.models_config import get_active_models, CONSENSUS_CONFIG
    
    active_models = get_active_models()
    print(f"   ü§ñ Modelos disponibles para consenso: {active_models}")
    
    # Simular consultas a m√∫ltiples modelos
    test_prompt = "¬øQu√© opinas sobre la inteligencia artificial?"
    
    print(f"   üìù Consulta: '{test_prompt}'")
    print(f"   ‚öñÔ∏è  M√©todo de consenso: {CONSENSUS_CONFIG['voting_method']}")
    
    # Simular respuestas de modelos (en una implementaci√≥n real, esto har√≠a llamadas reales)
    print("   üîÑ Simulando respuestas de modelos:")
    for model in active_models[:3]:  # Solo tomar algunos modelos para la simulaci√≥n
        print(f"     ‚Ä¢ {model}: respuesta simulada (2.5s, calidad alta)")
    
    # Aplicar l√≥gica de consenso
    if CONSENSUS_CONFIG['voting_method'] == 'weighted':
        weights = CONSENSUS_CONFIG['model_weights']
        print(f"   üìä Pesos aplicados: {weights}")
        
        # Simular selecci√≥n basada en pesos
        if len(active_models) >= CONSENSUS_CONFIG['min_models']:
            print("   ‚úÖ Condici√≥n de consenso satisfecha (m√≠n. modelos disponibles)")
            print("   üéØ Resultado de consenso: respuesta combinada usando pesos")
        else:
            print("   ‚ö†Ô∏è  No hay suficientes modelos para consenso")
            print(f"   üîÑ Usando modelo fallback: {CONSENSUS_CONFIG['fallback_model']}")
    
    return True

def main():
    """Funci√≥n principal de pruebas"""
    print("üß™ Pruebas locales del sistema Capibara6")
    print("   Router Sem√°ntico y Sistema de Consenso")
    print("=" * 60)
    
    success = True
    
    try:
        # Prueba 1: Configuraci√≥n de modelos
        success &= test_local_model_config()
        
        # Prueba 2: Plantillas de prompts
        success &= test_prompt_templates()
        
        # Prueba 3: L√≥gica de consenso
        success &= test_consenso_logic()
        
        # Prueba 4: Formateo de prompts
        success &= test_format_prompt()
        
        # Prueba 5: L√≥gica de enrutamiento
        success &= test_model_routing_logic()
        
        # Prueba 6: Simulaci√≥n de consenso (asincr√≥nica)
        asyncio.run(test_consensus_simulation())
        
        print("\n" + "=" * 60)
        print("üìã Resumen de pruebas locales:")
        print("   ‚úÖ Configuraci√≥n de modelos: Verificada")
        print("   ‚úÖ Plantillas de prompts: Verificadas")
        print("   ‚úÖ L√≥gica de consenso: Verificada")
        print("   ‚úÖ Formateo de prompts: Verificado")
        print("   ‚úÖ L√≥gica de enrutamiento: Verificada")
        print("   ‚úÖ Simulaci√≥n de consenso: Completada")
        
        if success:
            print("\n‚úÖ ¬°Todas las pruebas locales se completaron exitosamente!")
            print("\nüöÄ El sistema de router sem√°ntico y consenso est√° correctamente configurado con:")
            print("   - phi4: Modelo r√°pido para tareas simples")
            print("   - qwen2.5-coder: Modelo experto en c√≥digo y tareas t√©cnicas") 
            print("   - gpt-oss-20b: Modelo complejo para razonamiento avanzado")
            print("   - mixtral: Modelo general para tareas creativas")
            print("   - Sistema de consenso con votaci√≥n ponderada")
            print("   - Templates de prompts por categor√≠a")
            print("   - L√≥gica de enrutamiento sem√°ntico")
            
            return True
        else:
            print("\n‚ùå Hubo errores en algunas pruebas")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Error durante las pruebas: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)