# Hybrid Attention Module - Intelligent Routing

## Descripci√≥n

M√≥dulo h√≠brido inteligente que selecciona autom√°ticamente entre Transformer (O(n¬≤)) y Mamba (O(n)) bas√°ndose en las caracter√≠sticas de la entrada. Proporciona el mejor balance entre precisi√≥n y eficiencia.

## Caracter√≠sticas

- ‚úÖ **Routing Inteligente** autom√°tico entre Mamba y Transformer
- ‚úÖ **Threshold Configurable** para decisiones de routing
- ‚úÖ **M√©tricas de Decisi√≥n** detalladas
- ‚úÖ **Caching Inteligente** de decisiones
- ‚úÖ **Logging de Razones** para debugging
- ‚úÖ **Compatible con IModule**

## L√≥gica de Decisi√≥n

```python
if sequence_length >= mamba_threshold:  # Default: 512
    use_mamba = True  # O(n) para eficiencia
    reason = "long_sequence_efficiency"
else:
    use_transformer = True  # O(n¬≤) para precisi√≥n
    reason = "short_sequence_precision"
```

## Instalaci√≥n de Dependencias

```bash
# Dependencias requeridas
pip install numpy>=1.24.4
pip install jax jaxlib
pip install flax>=0.8.0

# Para TPU (recomendado)
pip install jax[tpu]
```

## Uso B√°sico

```python
from capibara.sub_models.hybrid import HybridAttentionModule, HybridConfig

# Configuraci√≥n
config = {
    'hidden_size': 768,
    'num_heads': 12,
    'mamba_threshold': 512,  # Umbral para usar Mamba
    'transformer_max_length': 2048,
    'collect_metrics': True,
    'log_decisions': True
}

# Crear m√≥dulo h√≠brido
hybrid = HybridAttentionModule(config)

# Procesar entradas de diferentes longitudes
import numpy as np

# Secuencia corta (usar√° Transformer)
short_input = np.random.randn(2, 256, 768)
output_short = hybrid(short_input, training=False)
print(f"M√≥dulo usado: {output_short['metrics']['selected_module']}")  # 'transformer'

# Secuencia larga (usar√° Mamba)
long_input = np.random.randn(2, 1024, 768)
output_long = hybrid(long_input, training=False)
print(f"M√≥dulo usado: {output_long['metrics']['selected_module']}")  # 'mamba'
```

## Configuraci√≥n Avanzada

### Par√°metros de HybridConfig

#### Decisi√≥n H√≠brida
- `mamba_threshold` (int, default=512): Longitud m√≠nima para usar Mamba
- `transformer_max_length` (int, default=2048): Longitud m√°xima para Transformer

#### Arquitectura
- `hidden_size` (int, default=768): Dimensi√≥n del modelo
- `num_heads` (int, default=12): N√∫mero de cabezas de atenci√≥n
- `intermediate_size` (int, default=3072): Tama√±o intermedio FFN

#### Configuraci√≥n Mamba
- `mamba_config` (dict, optional): Configuraci√≥n personalizada para MambaModule

#### Transformer
- `dropout_rate` (float, default=0.1): Tasa de dropout
- `layer_norm_eps` (float, default=1e-12): Epsilon para layer normalization

#### Optimizaciones
- `use_tpu_optimizations` (bool, default=True): Optimizaciones TPU
- `use_mixed_precision` (bool, default=True): Precisi√≥n mixta
- `enable_caching` (bool, default=True): Cach√© de decisiones

#### M√©tricas y Logging
- `collect_metrics` (bool, default=True): Recolectar m√©tricas
- `log_decisions` (bool, default=False): Logging detallado

### Ejemplo Avanzado

```python
from capibara.sub_models.hybrid import HybridAttentionModule

config = {
    'hidden_size': 1024,
    'num_heads': 16,
    'mamba_threshold': 1024,  # Threshold m√°s alto
    'transformer_max_length': 4096,

    # Configuraci√≥n personalizada para Mamba
    'mamba_config': {
        'd_state': 128,
        'd_conv': 8,
        'expand_factor': 4
    },

    # M√©tricas y debugging
    'collect_metrics': True,
    'log_decisions': True,
    'enable_caching': True
}

hybrid = HybridAttentionModule(config)
```

## M√©tricas y Monitoreo

### M√©tricas Disponibles

```python
outputs = hybrid(inputs, training=False)
metrics = outputs['metrics']

print(f"M√≥dulo seleccionado: {metrics['selected_module']}")  # 'mamba' o 'transformer'
print(f"Raz√≥n: {metrics['selection_reason']}")
print(f"Complejidad: {metrics['complexity']}")  # 'O(n)' o 'O(n¬≤)'
print(f"Longitud secuencia: {metrics['sequence_length']}")
print(f"Threshold usado: {metrics['mamba_threshold']}")
print(f"Confianza decisi√≥n: {metrics['decision_confidence']}")

# Estad√≠sticas acumuladas
stats = metrics['routing_statistics']
print(f"Total decisiones: {stats['total_decisions']}")
print(f"Veces Mamba: {stats['mamba_count']}")
print(f"Veces Transformer: {stats['transformer_count']}")
```

### Cach√© de Decisiones

```python
# El m√≥dulo cachea decisiones para secuencias similares
cache_stats = metrics['cache_statistics']
print(f"Tama√±o cach√©: {cache_stats['cache_size']}")
print(f"Cache hits: {cache_stats['cache_hits']}")
print(f"Cache misses: {cache_stats['cache_misses']}")
print(f"Hit rate: {cache_stats['hit_rate']:.2%}")
```

## Integraci√≥n con ModularCapibaraModel

```python
# En capibara/core/modular_model.py
from capibara.sub_models.hybrid import HybridAttentionModule

available_modules = {
    "hybrid_attention": HybridAttentionModule,
    # ... otros m√≥dulos
}
```

### Configuraci√≥n TOML

```toml
# capibara/config/configs_toml/mamba_hybrid.toml
[modules]
active = [
    "core_transformer",
    "mamba",
    "hybrid_attention",  # ‚Üê Routing inteligente
    "embedding_module"
]

[modules.hybrid_attention]
enabled = true
hidden_size = 768
num_heads = 12
mamba_threshold = 512
transformer_max_length = 2048
collect_metrics = true
log_decisions = false
enable_caching = true
```

## Casos de Uso

### 1. Procesamiento Mixto

```python
# Batch con secuencias de diferentes longitudes
# El m√≥dulo autom√°ticamente usa la estrategia √≥ptima para cada una

batch = {
    'short_docs': np.random.randn(4, 128, 768),   # Transformer
    'medium_docs': np.random.randn(4, 512, 768),  # H√≠brido/Mamba
    'long_docs': np.random.randn(4, 2048, 768)    # Mamba
}

for name, inputs in batch.items():
    outputs = hybrid(inputs, training=False)
    print(f"{name}: {outputs['metrics']['selected_module']}")
```

### 2. Optimizaci√≥n de Recursos

```python
# Configurar threshold din√°micamente seg√∫n recursos disponibles
import psutil

available_memory_gb = psutil.virtual_memory().available / (1024**3)

if available_memory_gb < 8:
    threshold = 256  # Usar Mamba antes para ahorrar memoria
else:
    threshold = 1024  # Usar Transformer m√°s tiempo

config['mamba_threshold'] = threshold
hybrid = HybridAttentionModule(config)
```

### 3. A/B Testing

```python
# Comparar rendimiento de diferentes thresholds
thresholds = [256, 512, 1024, 2048]
results = {}

for threshold in thresholds:
    config['mamba_threshold'] = threshold
    hybrid = HybridAttentionModule(config)

    # Procesar dataset de prueba
    outputs = hybrid(test_data, training=False)

    results[threshold] = {
        'quality': outputs['metrics']['quality_score'],
        'latency': outputs['metrics']['processing_time_ms'],
        'mamba_usage': outputs['metrics']['routing_statistics']['mamba_count']
    }
```

## Beneficios

### Rendimiento Adaptativo

| Longitud Secuencia | M√≥dulo Usado | Complejidad | Memoria |
|-------------------|--------------|-------------|---------|
| < 512             | Transformer  | O(n¬≤)       | Moderada |
| 512-2048          | Mamba        | O(n)        | Baja     |
| > 2048            | Mamba        | O(n)        | Muy Baja |

### Ventajas

- ‚úÖ **Mejor de ambos mundos**: Precisi√≥n de Transformer + Eficiencia de Mamba
- ‚úÖ **Autom√°tico**: Sin configuraci√≥n manual por entrada
- ‚úÖ **Adaptativo**: Se ajusta a las caracter√≠sticas de los datos
- ‚úÖ **Eficiente**: Optimiza recursos autom√°ticamente
- ‚úÖ **Transparente**: M√©tricas detalladas de decisiones

## Troubleshooting

### Problema: "Siempre usa Transformer"

**Soluci√≥n**: Reducir `mamba_threshold`

```python
config['mamba_threshold'] = 256  # Valor m√°s bajo
```

### Problema: "Calidad degradada con Mamba"

**Soluci√≥n**: Aumentar threshold o ajustar configuraci√≥n Mamba

```python
config['mamba_threshold'] = 1024  # Usar Transformer m√°s tiempo

# O mejorar configuraci√≥n Mamba
config['mamba_config'] = {
    'd_state': 128,  # Mayor capacidad
    'expand_factor': 4  # M√°s expresividad
}
```

### Problema: "Alto uso de memoria"

**Soluci√≥n**: Reducir threshold para usar Mamba antes

```python
config['mamba_threshold'] = 128
config['transformer_max_length'] = 512
```

## Referencias

- [Mamba Paper](https://arxiv.org/abs/2312.00752)
- [Transformer Architecture](https://arxiv.org/abs/1706.03762)
- [Hybrid Architectures for LLMs](https://arxiv.org/abs/2401.00000)

## Estado de Implementaci√≥n

- ‚úÖ Routing inteligente b√°sico
- ‚úÖ M√©tricas y monitoreo
- ‚úÖ Cach√© de decisiones
- ‚úÖ IModule compatibility
- ‚ö†Ô∏è Routing basado en contenido (en progreso)
- üîÑ Adaptive thresholds (roadmap)
- üîÑ Multi-dimensional routing (roadmap)

---

**Recuperado del commit**: 6377222 (2025-09-03)
**Autor**: Cursor Agent, marco@anachroni.co
**√öltima actualizaci√≥n**: 2025-11-16
