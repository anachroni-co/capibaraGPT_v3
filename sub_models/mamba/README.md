# Mamba Module - Selective State Space Model

## Descripci√≥n

Implementaci√≥n de Mamba (Selective State Space Model) para CapibaraGPT-v2. Proporciona procesamiento con complejidad **O(n)** en lugar de O(n¬≤) de Transformers tradicionales, ideal para secuencias largas.

## Caracter√≠sticas

- ‚úÖ **Complejidad O(n)** para procesamiento de secuencias
- ‚úÖ **Selective State Space Model** con par√°metros adaptativos
- ‚úÖ **Compatible con IModule** para integraci√≥n modular
- ‚úÖ **Optimizaciones TPU** con scan asociativo
- ‚úÖ **Fallbacks robustos** cuando JAX no est√° disponible
- ‚úÖ **M√©tricas detalladas** de complejidad y rendimiento

## Instalaci√≥n de Dependencias

```bash
# Instalar dependencias requeridas
pip install numpy>=1.24.4
pip install jax jaxlib
pip install flax>=0.8.0

# Para TPU (opcional pero recomendado)
pip install jax[tpu]
```

## Uso B√°sico

```python
from capibara.sub_models.mamba import MambaModule, MambaConfig

# Configuraci√≥n
config = {
    'hidden_size': 768,
    'd_state': 64,         # Dimensi√≥n del estado SSM
    'd_conv': 4,           # Kernel convoluci√≥n 1D
    'expand_factor': 2,    # Factor de expansi√≥n
    'scan_type': 'associative'  # Para paralelizaci√≥n TPU
}

# Crear m√≥dulo
mamba = MambaModule(config)

# Procesar entrada
import numpy as np
inputs = np.random.randn(2, 512, 768)  # [batch, seq_len, hidden_size]
outputs = mamba(inputs, training=False)

print(f"Complejidad: {outputs['metrics']['complexity']}")
print(f"Output shape: {outputs['output'].shape}")
```

## Configuraci√≥n Avanzada

### Par√°metros de MambaConfig

- `hidden_size` (int, default=768): Dimensi√≥n del modelo
- `d_state` (int, default=64): Dimensi√≥n del estado interno SSM
- `d_conv` (int, default=4): Tama√±o del kernel de convoluci√≥n 1D
- `expand_factor` (int, default=2): Factor de expansi√≥n para proyecciones
- `dt_rank` (int, default=32): Rango para par√°metro temporal Œî
- `activation` (str, default='swish'): Funci√≥n de activaci√≥n (swish, gelu, relu)
- `use_tpu_optimizations` (bool, default=True): Habilitar optimizaciones TPU
- `scan_type` (str, default='associative'): Tipo de scan ('linear' o 'associative')

### Ejemplo con Configuraci√≥n Personalizada

```python
from capibara.sub_models.mamba import MambaModule

config = {
    'hidden_size': 1024,
    'd_state': 128,
    'd_conv': 8,
    'expand_factor': 4,
    'activation': 'gelu',
    'use_tpu_optimizations': True,
    'scan_type': 'associative'
}

mamba = MambaModule(config)
```

## Integraci√≥n con ModularCapibaraModel

El m√≥dulo est√° dise√±ado para integrarse directamente con la arquitectura modular de Capibara:

```python
# En capibara/core/modular_model.py
from capibara.sub_models.mamba import MambaModule

available_modules = {
    "mamba": MambaModule,
    # ... otros m√≥dulos
}
```

### Configuraci√≥n TOML

```toml
# En capibara/config/configs_toml/mamba_hybrid.toml
[modules]
active = [
    "mamba",
    "embedding_module",
    # ... otros m√≥dulos
]

[modules.mamba]
enabled = true
hidden_size = 768
d_state = 64
d_conv = 4
expand_factor = 2
scan_type = "associative"
```

## M√©tricas y Monitoreo

El m√≥dulo proporciona m√©tricas detalladas:

```python
outputs = mamba(inputs, training=False)
metrics = outputs['metrics']

print(f"Mamba activo: {metrics['mamba_active']}")
print(f"Complejidad: {metrics['complexity']}")  # 'O(n)' o 'O(log n)'
print(f"Longitud secuencia: {metrics['sequence_length']}")
print(f"Dimensi√≥n estado: {metrics['d_state']}")
print(f"Selective scan usado: {metrics['selective_scan_used']}")
print(f"Optimizado TPU: {metrics['tpu_optimized']}")
```

## Performance

### Comparaci√≥n de Complejidad

| Longitud Secuencia | Transformer (O(n¬≤)) | Mamba (O(n)) | Mejora |
|-------------------|---------------------|--------------|--------|
| 512               | 262,144 ops         | 512 ops      | 512x   |
| 2048              | 4,194,304 ops       | 2048 ops     | 2048x  |
| 4096              | 16,777,216 ops      | 4096 ops     | 4096x  |

### Benchmarks Esperados

```
# Con TPU v4-32
- Throughput: ~3000 tokens/sec para secuencias de 2048 tokens
- Memoria: 4x menos que Transformer para secuencias > 1024
- Latencia: Sub-linear scaling con longitud de secuencia
```

## Troubleshooting

### Error: "JAX no disponible"

```bash
# Instalar JAX
pip install jax jaxlib

# Para TPU
pip install jax[tpu]
```

### Error: "Flax no disponible"

```bash
pip install flax>=0.8.0
```

### Modo Fallback

Si JAX no est√° disponible, el m√≥dulo usar√° una implementaci√≥n fallback con numpy:

```python
# El m√≥dulo detecta autom√°ticamente y usa fallback
# Se registrar√° un warning: "Usando implementaci√≥n fallback de Mamba"
```

## Referencias

- [Mamba: Linear-Time Sequence Modeling](https://arxiv.org/abs/2312.00752)
- [Structured State Space Models (S4)](https://arxiv.org/abs/2111.00396)
- [Selective State Space Models](https://github.com/state-spaces/mamba)

## Estado de Implementaci√≥n

- ‚úÖ Core SSM implementation
- ‚úÖ Selective scan mechanism
- ‚úÖ IModule interface compatibility
- ‚úÖ TPU optimizations (associative scan)
- ‚úÖ Fallback mode (numpy)
- ‚ö†Ô∏è Optimizaci√≥n completa de convoluci√≥n 1D (en progreso)
- üîÑ Mamba-2 features (roadmap)

## Contribuci√≥n

Para contribuir a la mejora del m√≥dulo Mamba:

1. Optimizaciones de conv1d para producci√≥n
2. Implementaci√≥n de Mamba-2 features
3. Benchmarks adicionales en diferentes hardware
4. Mejoras en el sistema de m√©tricas

---

**Recuperado del commit**: 6377222 (2025-09-03)
**Autor**: Cursor Agent, marco@anachroni.co
**√öltima actualizaci√≥n**: 2025-11-16
