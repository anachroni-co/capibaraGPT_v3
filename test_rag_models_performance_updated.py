#!/usr/bin/env python3
"""
Script de prueba actualizado para evaluar el rendimiento del sistema RAG
con cada modelo individualmente, incluyendo traducci√≥n al espa√±ol
"""

import requests
import time
import json
from typing import Dict, List, Tuple, Optional
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor


class RAGModelsTester:
    """Tester para evaluar el rendimiento de RAG con cada modelo"""
    
    def __init__(self, base_url: str = "http://localhost:8082"):
        self.base_url = base_url
        self.models = [
            'phi4_fast',
            'mistral_balanced', 
            'qwen_coder',
            'gemma3_multimodal',
            'aya_expanse_multilingual'
        ]
        
        # Consultas de prueba que probablemente activar√≠an RAG
        self.rag_queries = [
            "What is the latest research on quantum computing?",
            "Explain the technical details of ARM architecture optimization",
            "Tell me about recent developments in machine learning"
        ]
        
        # Consultas que requieren traducci√≥n
        self.translation_queries = [
            "Please translate 'Hello world' to Spanish with explanation",
            "Explain quantum physics in Spanish", 
            "What are the benefits of renewable energy in Spanish"
        ]

    def get_model_info(self) -> Dict:
        """Obtener informaci√≥n sobre los modelos disponibles"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"‚ùå Error al obtener modelos: {response.status_code}")
                return {}
        except Exception as e:
            print(f"‚ùå Error de conexi√≥n: {e}")
            return {}

    def get_loaded_models(self) -> List[str]:
        """Obtener solo los modelos que est√°n cargados"""
        try:
            response = requests.get(f"{self.base_url}/stats", timeout=10)
            if response.status_code == 200:
                stats = response.json()
                return stats.get("models_loaded", [])
            else:
                print(f"‚ùå Error al obtener stats: {response.status_code}")
                return []
        except Exception as e:
            print(f"‚ùå Error de conexi√≥n a stats: {e}")
            return []

    def test_model_rag(
        self, 
        model_id: str, 
        query: str, 
        translate: bool = True
    ) -> Dict:
        """Probar un modelo espec√≠fico con RAG y opcionalmente traducci√≥n"""
        try:
            start_time = time.time()
            
            # Preparar el mensaje, opcionalmente incluyendo instrucci√≥n de traducci√≥n
            messages = [{"role": "user", "content": query}]
            
            if translate:
                # A√±adir instrucci√≥n de traducci√≥n al espa√±ol
                translation_msg = f"{query}\n\nPor favor responde en espa√±ol."
                messages = [{"role": "user", "content": translation_msg}]
            
            payload = {
                "model": model_id,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 300,
                "stream": False
            }
            
            # Hacer la solicitud al servidor
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=120
            )
            
            total_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result['choices'][0]['message']['content']
                
                return {
                    "success": True,
                    "model": model_id,
                    "query": query,
                    "response": generated_text,
                    "total_time": total_time,
                    "tokens_used": result.get('usage', {}).get('total_tokens', 0),
                    "translate": translate
                }
            else:
                return {
                    "success": False,
                    "model": model_id,
                    "query": query,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "total_time": total_time,
                    "translate": translate
                }
                
        except Exception as e:
            total_time = time.time() - start_time
            return {
                "success": False,
                "model": model_id,
                "query": query,
                "error": str(e),
                "total_time": total_time,
                "translate": translate
            }

    def run_comprehensive_test(self):
        """Correr pruebas completas con y sin traducci√≥n"""
        print("üöÄ Iniciando pruebas de rendimiento RAG por modelo")
        print("="*80)
        
        # Obtener modelos disponibles
        models_info = self.get_model_info()
        if not models_info:
            print("‚ùå No se pudieron obtener los modelos disponibles")
            return
        
        available_models = [model['id'] for model in models_info.get('data', [])]
        print(f"‚úÖ Modelos disponibles: {available_models}")
        
        # Obtener modelos cargados
        loaded_models = self.get_loaded_models()
        print(f"üì¶ Modelos cargados actualmente: {loaded_models}")
        
        # Filtrar modelos que est√°n disponibles y cargarlos si es necesario
        all_models_to_test = available_models

        results = []
        
        # Probar cada modelo con cada tipo de consulta
        for model in all_models_to_test:
            print(f"\nü§ñ Probando modelo: {model}")
            print("-" * 50)
            
            # Probar con consultas RAG (sin traducci√≥n)
            print("üîç Consultas RAG (sin traducci√≥n):")
            for i, query in enumerate(self.rag_queries[:2]):  # Limitar a 2 por modelo
                print(f"  Query {i+1}: {query[:50]}...")
                result = self.test_model_rag(model, query, translate=False)
                results.append(result)
                
                if result['success']:
                    print(f"    ‚úÖ {result['total_time']:.2f}s | Tokens: {result.get('tokens_used', 'N/A')}")
                    # Mostrar una muestra de la respuesta
                    print(f"    üìù Muestra: {result['response'][:100]}...")
                else:
                    print(f"    ‚ùå Error: {result['error']}")
            
            # Probar con consultas de traducci√≥n
            print("üåé Consultas con traducci√≥n al espa√±ol:")
            for i, query in enumerate(self.translation_queries[:2]):  # Limitar a 2 por modelo
                print(f"  Query {i+1}: {query[:50]}...")
                result = self.test_model_rag(model, query, translate=True)
                results.append(result)
                
                if result['success']:
                    print(f"    ‚úÖ {result['total_time']:.2f}s | Tokens: {result.get('tokens_used', 'N/A')}")
                    # Mostrar una muestra de la respuesta
                    print(f"    üìù Muestra: {result['response'][:100]}...")
                else:
                    print(f"    ‚ùå Error: {result['error']}")
        
        # Mostrar resumen
        self.print_summary(results)
        return results

    def print_summary(self, results: List[Dict]):
        """Imprimir resumen de resultados"""
        print("\n" + "="*80)
        print("üìä RESUMEN DE RESULTADOS")
        print("="*80)
        
        # Agrupar por modelo
        model_results = {}
        for result in results:
            model = result['model']
            if model not in model_results:
                model_results[model] = []
            model_results[model].append(result)
        
        for model, model_tests in model_results.items():
            print(f"\nü§ñ {model}:")
            print(f"  Total pruebas: {len(model_tests)}")
            
            successful_tests = [t for t in model_tests if t['success']]
            if successful_tests:
                avg_time = sum(t['total_time'] for t in successful_tests) / len(successful_tests)
                avg_tokens = sum(t.get('tokens_used', 0) for t in successful_tests) / len(successful_tests) if successful_tests else 0
                
                print(f"  √âxito: {len(successful_tests)}/{len(model_tests)}")
                print(f"  Tiempo promedio: {avg_time:.2f}s")
                print(f"  Tokens promedio: {avg_tokens:.1f}")
                
                # Separar por tipo de consulta
                rag_tests = [t for t in successful_tests if not t['translate']]
                if rag_tests:
                    avg_rag_time = sum(t['total_time'] for t in rag_tests) / len(rag_tests)
                    print(f"  - RAG (sin traducci√≥n): {avg_rag_time:.2f}s promedio")
                
                translation_tests = [t for t in successful_tests if t['translate']]
                if translation_tests:
                    avg_trans_time = sum(t['total_time'] for t in translation_tests) / len(translation_tests)
                    print(f"  - Traducci√≥n (al espa√±ol): {avg_trans_time:.2f}s promedio")
                    
                    # Mostrar ejemplo de traducci√≥n exitosa si hay una
                    if len(translation_tests) > 0:
                        sample_response = translation_tests[0]['response']
                        print(f"  - Ejemplo de traducci√≥n:")
                        print(f"    '{sample_response[:200]}...'")
            else:
                print(f"  ‚ùå Todos los tests fallaron")
                
                # Para modelos que fallaron, mostrar el tipo de error m√°s com√∫n
                if model_tests:
                    errors = [t.get('error', 'Unknown error') for t in model_tests]
                    # Encontrar el error m√°s com√∫n
                    error_counts = {}
                    for error in errors:
                        error_counts[error] = error_counts.get(error, 0) + 1
                    
                    most_common_error = max(error_counts, key=error_counts.get)
                    print(f"  - Error m√°s com√∫n: {most_common_error[:100]}...")
        
        # Estad√≠sticas generales
        all_successful = [r for r in results if r['success']]
        if all_successful:
            total_avg_time = sum(r['total_time'] for r in all_successful) / len(all_successful)
            print(f"\nüìà ESTAD√çSTICAS GLOBALES:")
            print(f"  Total pruebas: {len(results)}")
            print(f"  Pruebas exitosas: {len(all_successful)}")
            print(f"  Tasa de √©xito: {len(all_successful)/len(results)*100:.1f}%")
            print(f"  Tiempo promedio total: {total_avg_time:.2f}s")
        else:
            print(f"\n‚ùå No se complet√≥ ninguna prueba exitosamente")
        
        print("\n" + "="*80)

    def save_results(self, results: List[Dict], filename: str = "rag_models_test_results.json"):
        """Guardar resultados en archivo JSON"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ Resultados guardados en {filename}")
        except Exception as e:
            print(f"‚ùå Error guardando resultados: {e}")


def main():
    """Funci√≥n principal"""
    print("üß™ TEST DE RENDIMIENTO RAG POR MODELO")
    print("="*80)
    print("Este script probar√°:")
    print("- Cada modelo individualmente")
    print("- Consultas RAG (sin traducci√≥n)")
    print("- Consultas con traducci√≥n autom√°tica al espa√±ol")
    print("- Medici√≥n de latencias y tokens")
    print("- Comparaci√≥n de rendimiento entre modelos")
    print("="*80)
    
    # Crear tester
    tester = RAGModelsTester()
    
    # Correr pruebas
    results = tester.run_comprehensive_test()
    
    # Guardar resultados
    tester.save_results(results)
    
    print("\n‚úÖ Pruebas completadas")


if __name__ == "__main__":
    main()