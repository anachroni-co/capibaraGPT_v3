# GuÃ­a RÃ¡pida - Capibara6 ARM Axion

## Para Agentes y Desarrolladores

Esta es una guÃ­a rÃ¡pida para entender el sistema actual y ponerse en marcha rÃ¡pidamente.

## âš ï¸ InformaciÃ³n Importante para Agentes - Leer Primero

**Arquitectura Distribuida - VM models-europe:**
- Esta VM (`models-europe`) SOLO debe ejecutar servicios de IA/modelos
- NO iniciar servicios MCP, TTS o backend en esta VM
- Servicios como `mcp_server.py`, `kyutai_tts_server.py`, `capibara6_integrated_server.py` corren en la VM `services`
- Esta VM ejecuta: `multi_model_server.py` en el puerto 8082 con 5 modelos de IA

## âš¡ Estado Actual (2025-12-02)

**Sistema**: âœ… Operativo
**Servidor**: Puerto 8082
**Modelos**: 5 modelos disponibles (con lazy loading)
**VM**: models-europe (ARM Axion C4A-standard-32)

## ğŸš€ Inicio RÃ¡pido en 30 Segundos

```bash
# 1. Verificar que el servidor estÃ¡ corriendo
curl http://localhost:8082/health

# 2. Si no estÃ¡ corriendo, iniciarlo
cd /home/elect/capibara6/arm-axion-optimizations/vllm_integration
python3 multi_model_server.py --host 0.0.0.0 --port 8082 --config config.json

# 3. Verificar modelos disponibles
curl http://localhost:8082/v1/models | jq '.data[].id'
```

## ğŸ“š DocumentaciÃ³n Esencial

**LEE ESTOS ARCHIVOS EN ESTE ORDEN:**

1. **`README.md`** (este directorio)
   - Estado actual completo del sistema
   - Arquitectura y componentes
   - Ejemplos de uso

2. **`arm-axion-optimizations/vllm_integration/README.md`**
   - DocumentaciÃ³n tÃ©cnica del servidor
   - API completa
   - Troubleshooting

3. **`PRODUCTION_ARCHITECTURE.md`**
   - Arquitectura distribuida entre VMs
   - ComunicaciÃ³n entre servicios

## ğŸ¤– Modelos Disponibles

1. **phi4_fast** â†’ Respuestas rÃ¡pidas y simples
2. **mistral_balanced** â†’ Tareas tÃ©cnicas intermedias
3. **qwen_coder** â†’ Especializado en cÃ³digo
4. **gemma3_multimodal** â†’ AnÃ¡lisis complejo, imÃ¡genes
5. **aya_expanse_multilingual** â†’ 23 idiomas, multilingÃ¼e

**Nota**: Los modelos usan **lazy loading**. Primera carga tarda 20-60 segundos.

## ğŸ” Verificaciones RÃ¡pidas

```bash
# Â¿EstÃ¡ el servidor corriendo?
ps aux | grep multi_model_server

# Â¿Puerto 8082 estÃ¡ escuchando?
ss -tlnp | grep 8082

# Â¿CuÃ¡nta memoria hay disponible?
free -h

# Ver logs del servidor
tail -50 /tmp/multi_model_server.log

# Verificar configuraciÃ³n
cd /home/elect/capibara6/arm-axion-optimizations/vllm_integration
cat config.json | jq '.experts | length'
# Debe devolver: 5
```

## ğŸ§ª Prueba RÃ¡pida

```bash
# Test simple
curl -X POST http://localhost:8082/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4_fast",
    "messages": [{"role": "user", "content": "Hola"}],
    "max_tokens": 20
  }'

# Test con router automÃ¡tico (sin especificar modelo)
curl -X POST http://localhost:8082/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Escribe una funciÃ³n Python"}],
    "max_tokens": 100
  }'
```

## ğŸ“ Estructura de Archivos

```
/home/elect/capibara6/
â”œâ”€â”€ README.md                         â† Lee esto primero
â”œâ”€â”€ QUICK_START.md                    â† Este archivo
â”œâ”€â”€ PRODUCTION_ARCHITECTURE.md        â† Arquitectura
â”‚
â”œâ”€â”€ arm-axion-optimizations/
â”‚   â””â”€â”€ vllm_integration/
â”‚       â”œâ”€â”€ README.md                 â† Docs tÃ©cnicas
â”‚       â”œâ”€â”€ multi_model_server.py     â† Servidor principal
â”‚       â””â”€â”€ config.json               â† ConfiguraciÃ³n (symlink)
â”‚
â”œâ”€â”€ backend/                          â† Backend services
â”œâ”€â”€ /home/elect/models/               â† Modelos (5 modelos)
â””â”€â”€ docs/deprecated/                  â† Docs antiguas (NO USAR)
```

## âš ï¸ Documentos Obsoletos

**NO USES** los documentos en `docs/deprecated/`. Son histÃ³ricos y estÃ¡n desactualizados.

Si un documento no estÃ¡ en esta lista, probablemente estÃ¡ obsoleto:
- âœ… README.md
- âœ… QUICK_START.md (este archivo)
- âœ… PRODUCTION_ARCHITECTURE.md
- âœ… README_MODELS_SETUP.md
- âœ… AYA_EXPANSE_MODEL_CONFIRMATION.md
- âœ… arm-axion-optimizations/vllm_integration/README.md

## ğŸ› ï¸ Comandos Comunes

### Iniciar Servidor

```bash
cd /home/elect/capibara6/arm-axion-optimizations/vllm_integration
python3 multi_model_server.py --host 0.0.0.0 --port 8082 --config config.json
```

### Servicios por VM (Importante para Agentes)

**En VM models-europe (esta VM)** - Solo servicios de IA:
- âœ… `multi_model_server.py` en puerto 8082 (servidor de modelos con router semÃ¡ntico)
- âœ… 5 modelos de IA con optimizaciones ARM-Axion
- âŒ NO iniciar: MCP, TTS, servidores backend (corren en otras VMs)

**En VM services** - Servicios de backend y coordinaciÃ³n:
- âœ… `capibara6_integrated_server.py` (backend principal)
- âœ… `mcp_server.py` (Model Context Protocol en puerto 5003)
- âœ… `kyutai_tts_server.py` (Text-to-Speech en puerto 5002)
- âœ… `smart_mcp_server.py` (alternativa en puerto 5010)
- âŒ NO iniciar: Servidor de modelos vLLM (corre en models-europe)

**ComunicaciÃ³n entre VMs**:
- services â†’ models-europe: `http://34.175.48.2:8082` (API de modelos)
- frontend â†’ services: `http://34.175.255.139:5000/api/chat` (endpoint principal)

### Detener Servidor

```bash
# Encontrar PID
ps aux | grep multi_model_server | grep -v grep

# Matar proceso (reemplazar <PID> con el nÃºmero)
kill <PID>
```

### Cambiar ConfiguraciÃ³n

```bash
cd /home/elect/capibara6/arm-axion-optimizations/vllm_integration

# Ver configuraciÃ³n actual
ls -la config.json

# Cambiar a otra configuraciÃ³n
ln -sf config.five_models_optimized_with_aya.json config.json

# Reiniciar servidor para aplicar cambios
```

### Ver EstadÃ­sticas

```bash
# Health check
curl http://localhost:8082/health

# Modelos disponibles
curl http://localhost:8082/v1/models

# EstadÃ­sticas detalladas
curl http://localhost:8082/stats | jq
```

## ğŸ”§ ResoluciÃ³n RÃ¡pida de Problemas

### "Servidor no responde"

```bash
# 1. Verificar que estÃ¡ corriendo
ps aux | grep multi_model_server

# 2. Si no estÃ¡, iniciarlo
cd /home/elect/capibara6/arm-axion-optimizations/vllm_integration
python3 multi_model_server.py --host 0.0.0.0 --port 8082 --config config.json
```

### "Modelo tarda mucho"

- **Normal**: Primera carga (lazy loading) tarda 20-60 segundos
- **Espera**: Deja que termine de cargar
- **Siguiente vez**: SerÃ¡ instantÃ¡neo

### "Error de memoria"

```bash
# Ver memoria disponible
free -h

# Reducir modelos cargados simultÃ¡neamente
# Editar config.json y reducir max_loaded_experts de 5 a 3
```

## ğŸ“ Â¿Necesitas MÃ¡s Ayuda?

1. **Primero**: Lee `README.md` completo
2. **Luego**: Lee `arm-axion-optimizations/vllm_integration/README.md`
3. **Logs**: Revisa `/tmp/multi_model_server.log`
4. **Arquitectura**: Lee `PRODUCTION_ARCHITECTURE.md`

## ğŸ¯ Objetivo del Sistema

Sistema de IA conversacional con mÃºltiples modelos especializados que:
- Usa router semÃ¡ntico para seleccionar el mejor modelo automÃ¡ticamente
- Optimizado para ARM Axion con kernels NEON
- API compatible con OpenAI
- Lazy loading para eficiencia de memoria

---

**Ãšltima actualizaciÃ³n**: 2025-12-02
**Servidor**: Puerto 8082
**Estado**: âœ… Operativo
