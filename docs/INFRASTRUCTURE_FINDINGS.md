# Hallazgos de Infraestructura - Capibara6
## Fecha: 2025-11-13

## Resumen Ejecutivo

Investigaci√≥n completa de la infraestructura del proyecto Capibara6 para identificar todos los servicios, bases de datos y servidores activos o documentados.

---

## üîç Servicios Backend Encontrados y Verificados

### Servidor Principal: `backend/server_gptoss.py` (Puerto 5001)
**Rol:** Backend principal del chatbot con GPT-OSS-20B
**Estado:** ‚úÖ Activo y configurado
**Endpoints:**
- `POST /api/chat` - Chat b√°sico
- `POST /api/chat/stream` - Chat con streaming
- `GET /api/health` - Health check
- `GET /api/models` - Listar modelos
- `POST /api/save-conversation` - Guardar conversaciones

**Frontend conecta a:** `http://localhost:5001` (desarrollo) o `VM_MODELS:5001` (producci√≥n)

### Servidor FastAPI Alternativo: `backend/main.py` (Puerto 8000)
**Rol:** API alternativa con E2B integrado
**Estado:** ‚ö†Ô∏è Disponible pero no usado por frontend actual
**Endpoints:**
- `GET /health`
- `POST /api/v1/query` - Consulta al modelo
- `GET /api/v1/models` - Listar modelos
- `POST /api/v1/e2b/execute` - Ejecutar c√≥digo en E2B

### Servidor MCP: `backend/mcp_server.py` (Puerto 5003)
**Rol:** Model Context Protocol - RAG y contexto inteligente
**Estado:** ‚ö†Ô∏è Opcional, deshabilitado por defecto
**Endpoints:**
- `GET /api/mcp/contexts` - Listar contextos
- `GET /api/mcp/context/<id>` - Obtener contexto espec√≠fico
- `POST /api/mcp/augment` - Aumentar prompt con contexto (RAG)
- `GET /api/mcp/tools` - Listar herramientas
- `POST /api/mcp/calculate` - Calculadora
- `POST /api/mcp/verify` - Verificar hechos
- `GET /api/mcp/health` - Health check

**Configuraci√≥n Frontend:** `web/config.js` ‚Üí `SERVICES.MCP.enabled = false`

### Servidor TTS: `backend/kyutai_tts_server.py` (Puerto 5002)
**Rol:** Text-to-Speech con Kyutai Moshi
**Estado:** ‚úÖ Activo en VM gpt-oss-20b (34.175.136.104:5002)
**Endpoints:**
- `POST /tts` - S√≠ntesis de voz
- `GET /voices` - Listar voces
- `POST /clone` - Clonar voz
- `GET /health` - Health check
- `POST /preload` - Precargar modelo

**Documentaci√≥n:** `SERVICES_SETUP.md`

### Servidor Auth: `backend/auth_server.py` (Puerto 5004)
**Rol:** Autenticaci√≥n OAuth (GitHub y Google)
**Estado:** ‚úÖ Configurado
**Endpoints:**
- `GET /auth/github` - Login con GitHub
- `GET /auth/google` - Login con Google
- `POST /auth/verify` - Verificar token
- `POST /auth/logout` - Cerrar sesi√≥n
- `GET /auth/callback/github` - Callback GitHub
- `GET /auth/callback/google` - Callback Google
- `GET /health` - Health check

**Cambio reciente:** Puerto cambiado de 5001 ‚Üí 5004 (Fase 2)

### Servidor Consensus: `backend/consensus_server.py` (Puerto 5005)
**Rol:** Consenso multi-modelo
**Estado:** ‚ö†Ô∏è Deshabilitado por defecto
**Endpoints:**
- `POST /api/consensus/query` - Consulta con consenso
- `GET /api/consensus/models` - Listar modelos
- `GET /api/consensus/templates` - Templates de consenso
- `GET /api/consensus/config` - Configuraci√≥n
- `GET /api/consensus/health` - Health check

**Cambio reciente:** Puerto cambiado de 5002 ‚Üí 5005 (Fase 2)
**Configuraci√≥n Frontend:** `web/config.js` ‚Üí `SERVICES.CONSENSUS.enabled = false`

### Servidor Smart MCP Alternativo: `backend/smart_mcp_server.py` (Puerto 5010)
**Rol:** MCP alternativo con RAG selectivo simplificado
**Estado:** ‚ö†Ô∏è Opcional, alternativa a mcp_server.py
**Endpoints:**
- `GET /health` - Health check
- `POST /analyze` - An√°lisis de query
- `POST /update-date` - Actualizar fecha

**Configuraci√≥n Frontend:** `web/config.js` ‚Üí `SERVICES.SMART_MCP.enabled = false`

---

## üóÑÔ∏è Bases de Datos Encontradas

### PostgreSQL (Puerto 5432)
**Ubicaci√≥n:** `docker-compose.yml`
**Estado:** ‚úÖ Configurado en Docker
**Uso:** Base de datos principal para persistencia
```yaml
postgres:
  image: postgres:15
  ports:
    - "5432:5432"
  volumes:
    - postgres_data:/var/lib/postgresql/data
```

### TimescaleDB (Puerto 5433)
**Ubicaci√≥n:** `docker-compose.yml`
**Estado:** ‚úÖ Configurado en Docker
**Uso:** Time-series data (m√©tricas, logs temporales)
```yaml
timescaledb:
  image: timescale/timescaledb:latest-pg15
  ports:
    - "5433:5432"
  volumes:
    - timescale_data:/var/lib/postgresql/data
```

### Redis (Puerto 6379)
**Ubicaci√≥n:** `docker-compose.yml`
**Estado:** ‚úÖ Configurado en Docker
**Uso:** Cache y sesiones
```yaml
redis:
  image: redis:7-alpine
  ports:
    - "6379:6379"
  volumes:
    - redis_data:/data
```

### FAISS Vector Store
**Ubicaci√≥n:** `backend/config/infrastructure_config.py`
**Estado:** ‚úÖ Configurado como vector store principal
**Uso:** B√∫squeda de vectores para RAG
```python
RAG_CONFIG = {
    'vector_store': {
        'type': 'faiss',
        'index_type': 'IndexFlatIP',
        'embedding_dimension': 384
    }
}
```

### ChromaDB
**Ubicaci√≥n:** `archived/backend_modules/core/rag/vector_store.py`
**Estado:** ‚ùå Solo en c√≥digo archivado, no activo
**Nota:** C√≥digo existe pero no est√° en uso actualmente

---

## ‚úÖ Servicios Encontrados en VM rag3 (ACTUALIZACI√ìN 2025-11-13)

### Milvus Vector Database
**B√∫squeda en repositorio:** ‚ùå No encontrado en c√≥digo
**B√∫squeda en VM rag3:** ‚úÖ **ENCONTRADO Y ACTIVO**

**Ubicaci√≥n:** VM rag3 (europe-west2-c)
**Puerto:** 19530
**Implementaci√≥n:** Docker Compose en VM rag3
**Estado:** ‚úÖ Corriendo (uptime: 3 d√≠as)
**Versi√≥n:** v2.3.10
**Contenedor:** `milvus-standalone`

**Stack Completo:**
- Milvus server (puerto 19530, 9091)
- MinIO object storage (9000-9001)
- etcd coordination (2379-2380)

**Uso:** Vector database para RAG, b√∫squeda sem√°ntica, embeddings

### Nebula Graph Database
**B√∫squeda en repositorio:** ‚ùå No encontrado en c√≥digo
**B√∫squeda en VM rag3:** ‚úÖ **ENCONTRADO Y ACTIVO**

**Ubicaci√≥n:** VM rag3 (europe-west2-c)
**Puerto Principal:** 9669 (query service)
**Implementaci√≥n:** Docker Compose en VM rag3 (cluster de 3 nodos)
**Estado:** ‚úÖ Corriendo (uptime: 3 d√≠as)
**Versi√≥n:** v3.1.0

**Arquitectura del Cluster:**
- 3x nebula-graphd (query service - puerto 9669)
- 3x nebula-metad (metadata service - puerto 9559)
- 3x nebula-storaged (storage service - puerto 9779)
- 1x nebula-graph-studio (UI web - puerto 7001)

**Uso:** Graph database para relaciones complejas, knowledge graphs

### Servidor "Bridge" - capibara6-api
**B√∫squeda en repositorio:** ‚ùå No encontrado expl√≠citamente
**B√∫squeda en VM rag3:** ‚úÖ **ENCONTRADO Y ACTIVO**

**Ubicaci√≥n:** VM rag3 (europe-west2-c)
**Puerto:** 8000
**Implementaci√≥n:** Docker container `capibara6-api`
**Estado:** ‚úÖ Corriendo (uptime: 2 d√≠as)
**Comando:** `python -m uvicorn backend.main:app --host 0.0.0.0 --port 8000`

**Funci√≥n del Bridge:**
- ‚úÖ API principal de integraci√≥n
- ‚úÖ Coordina Milvus (vector search)
- ‚úÖ Coordina Nebula Graph (graph queries)
- ‚úÖ Orquesta 3x RQ workers para procesamiento as√≠ncrono
- ‚úÖ Integraci√≥n con PostgreSQL, TimescaleDB, Redis

**Workers Asociados:**
- capibara6-worker-1
- capibara6-worker-2
- capibara6-worker-3

**Nota sobre repositorio:**
El c√≥digo de `capibara6-api` debe estar en un directorio/imagen personalizada. El frontend puede integrarse directamente con este servicio en `http://rag3:8000`

### Rol de server_gptoss.py
**`backend/server_gptoss.py`** (puerto 5001 en VM bounty2) act√∫a como **backend secundario** para:
- Chat directo con GPT-OSS-20B
- TTS integration
- Autenticaci√≥n
- Consensus multi-modelo

Mientras que **`capibara6-api`** (puerto 8000 en VM rag3) es el **bridge principal** para:
- RAG con Milvus + Nebula
- Procesamiento as√≠ncrono
- Integraci√≥n completa del stack

**Evidencia:**
```javascript
// web/config.js
const CHATBOT_CONFIG = {
    BACKEND_URL: isLocalhost ? 'http://localhost:5001' : VM_MODELS + ':5001',
    ENDPOINTS: {
        CHAT: '/api/v1/query',
        CHAT_STREAM: '/api/v1/chat/stream',
        TTS_SPEAK: '/api/tts/speak',
        MCP_CONTEXT: '/api/v1/mcp/context',
        E2B_EXECUTE: '/api/v1/e2b/execute'
    }
}
```

---

## üìã Servicios Externos Documentados

### N8N Workflow Automation (Puerto 5678)
**Ubicaci√≥n:** VM gpt-oss-20b (34.175.136.104:5678)
**Estado:** ‚ö†Ô∏è Requiere VPN/t√∫nel
**Documentaci√≥n:** `SERVICES_SETUP.md`
**Configuraci√≥n Frontend:** Deshabilitado en `web/config.js` ‚Üí `N8N_ENABLED: false`

**Raz√≥n de deshabilitado:** No es accesible p√∫blicamente, requiere conexi√≥n VPN a la VM

### VM rag3 (europe-west2-c)
**Estado:** ‚úÖ **VERIFICADO Y DOCUMENTADO** (2025-11-13)
**Descripci√≥n:** Sistema RAG completo con vector + graph databases
**Documentaci√≥n completa:** Ver `VM_RAG3_COMPLETE_ANALYSIS.md`

**Servicios Principales:**
- ‚úÖ **Milvus Vector Database** (puerto 19530) - Vector search para RAG
- ‚úÖ **Nebula Graph Database** (puerto 9669) - Graph database (cluster 3 nodos)
- ‚úÖ **capibara6-api Bridge** (puerto 8000) - API principal de integraci√≥n
- ‚úÖ **PostgreSQL** (puerto 5432) - Base de datos relacional
- ‚úÖ **TimescaleDB** (puerto 5433) - Time-series data
- ‚úÖ **Redis** (puerto 6379) - Cache y queue
- ‚úÖ **N8N** (puerto 5678) - Workflow automation
- ‚úÖ **Nginx** (puertos 80, 443) - Reverse proxy
- ‚úÖ **Monitoring Stack** - Grafana (3000), Prometheus (9090), Jaeger (16686)

**Workers:**
- 3x RQ Workers para procesamiento as√≠ncrono en background

---

## üèóÔ∏è Arquitectura de VMs

### VM bounty2 (34.12.166.76)
**Servicios:**
- Backend principal (server_gptoss.py - puerto 5001)
- Auth server (puerto 5004)
- Consensus server (puerto 5005)
- Ollama (modelo local)

### VM gpt-oss-20b (34.175.136.104)
**Servicios:**
- TTS Server (puerto 5002)
- MCP Server (puerto 5003)
- Smart MCP alternativo (puerto 5010)
- N8N (puerto 5678 - VPN requerida)

### VM rag3 (direcci√≥n desconocida)
**Estado:** Mencionada en documentaci√≥n pero sin detalles
**Servicios esperados:**
- Sistema RAG completo
- ¬øMilvus?
- ¬øNebula Graph?
- ¬øBridge server?

---

## üîß Configuraci√≥n de RAG

### Mini RAG
```python
'mini_rag': {
    'timeout_ms': 50,
    'max_results': 5,
    'cache_size': 1000,
    'cache_ttl_seconds': 300
}
```

### Full RAG
```python
'full_rag': {
    'max_results': 10,
    'expansion_factor': 2.0,
    'deep_search_timeout_ms': 200
}
```

### Vector Store
```python
'vector_store': {
    'type': 'faiss',
    'index_type': 'IndexFlatIP',
    'embedding_dimension': 384
}
```

---

## üìä Resumen de Puertos - Arquitectura Completa

### VM bounty2 (34.12.166.76)
| Puerto | Servicio | Estado | Descripci√≥n |
|--------|----------|--------|-------------|
| 5001 | Backend Principal (server_gptoss.py) | ‚úÖ Activo | Chat GPT-OSS-20B |
| 5004 | Auth Server | ‚úÖ Configurado | OAuth GitHub/Google |
| 5005 | Consensus Server | ‚ö†Ô∏è Opcional | Multi-modelo |
| 8000 | FastAPI (main.py) | ‚ö†Ô∏è Alternativo | API E2B |

### VM gpt-oss-20b (34.175.136.104)
| Puerto | Servicio | Estado | Descripci√≥n |
|--------|----------|--------|-------------|
| 5002 | TTS Server (Kyutai) | ‚úÖ Activo | Text-to-Speech |
| 5003 | MCP Server | ‚ö†Ô∏è Opcional | Context & RAG |
| 5010 | Smart MCP Alternativo | ‚ö†Ô∏è Opcional | RAG selectivo |
| 5678 | N8N | ‚ö†Ô∏è VPN requerida | Workflows |

### VM rag3 (europe-west2-c) ‚≠ê NUEVO
| Puerto | Servicio | Estado | Descripci√≥n |
|--------|----------|--------|-------------|
| **80** | **Nginx HTTP** | ‚úÖ Activo | Reverse proxy |
| **443** | **Nginx HTTPS** | ‚úÖ Activo | Reverse proxy SSL |
| **3000** | **Grafana** | ‚úÖ Activo | Dashboards |
| **5432** | **PostgreSQL** | ‚úÖ Activo | DB Relacional |
| **5433** | **TimescaleDB** | ‚úÖ Activo | Time-series |
| **5678** | **N8N** | ‚úÖ Activo | Workflows |
| **6379** | **Redis** | ‚úÖ Activo | Cache + Queue |
| **7001** | **Nebula Studio** | ‚úÖ Activo | Graph UI |
| **8000** | **capibara6-api (BRIDGE)** | ‚úÖ **ACTIVO** | **API Principal** |
| **9000-9001** | **MinIO** | ‚úÖ Activo | Object Storage |
| **9090** | **Prometheus** | ‚úÖ Activo | Metrics |
| **9091** | **Milvus Metrics** | ‚úÖ Activo | Milvus stats |
| **9669** | **Nebula Graph Query** | ‚úÖ **ACTIVO** | **Graph DB** |
| **14268** | **Jaeger Collector** | ‚úÖ Activo | Tracing |
| **16686** | **Jaeger UI** | ‚úÖ Activo | Tracing UI |
| **19530** | **Milvus** | ‚úÖ **ACTIVO** | **Vector DB** |

### Puertos Locales (Docker Compose)
| Puerto | Servicio | Estado | VM |
|--------|----------|--------|-----|
| 5432 | PostgreSQL | ‚úÖ Docker | Local |
| 5433 | TimescaleDB | ‚úÖ Docker | Local |
| 6379 | Redis | ‚úÖ Docker | Local |

---

## üéØ Conclusiones - ACTUALIZADO 2025-11-13

### ‚úÖ Hallazgos Confirmados

1. **Backend principal** claramente definido (server_gptoss.py en puerto 5001 - VM bounty2)
2. **Bridge API confirmado** - capibara6-api en puerto 8000 (VM rag3)
3. **Milvus Vector Database** - ENCONTRADO y activo (puerto 19530 - VM rag3)
4. **Nebula Graph Database** - ENCONTRADO y activo (puerto 9669 - VM rag3, cluster completo)
5. **ChromaDB** - NO instalado (Milvus lo reemplaza)
6. Servicios especializados con puertos dedicados
7. Frontend correctamente configurado
8. **Stack de monitoreo completo** en VM rag3 (Grafana, Prometheus, Jaeger)

### üèóÔ∏è Arquitectura Verificada

El sistema Capibara6 utiliza **3 VMs especializadas**:

1. **VM bounty2** - Chat y modelos
   - Backend GPT-OSS-20B
   - Auth y Consensus

2. **VM gpt-oss-20b** - Servicios especializados
   - TTS (Kyutai)
   - MCP (RAG b√°sico)
   - N8N

3. **VM rag3** - Sistema RAG completo ‚≠ê
   - **Milvus** (vector search)
   - **Nebula Graph** (knowledge graph)
   - **capibara6-api** (bridge/orquestador)
   - PostgreSQL + TimescaleDB + Redis
   - Stack de monitoreo
   - 3x Workers para procesamiento as√≠ncrono

### ‚ö†Ô∏è √Åreas Completadas
1. ‚úÖ **VM rag3:** Completamente documentada (ver VM_RAG3_COMPLETE_ANALYSIS.md)
2. ‚úÖ **Milvus:** Encontrado y documentado (VM rag3:19530)
3. ‚úÖ **Nebula Graph:** Encontrado y documentado (VM rag3:9669)
4. ‚úÖ **Bridge Server:** Identificado como capibara6-api (VM rag3:8000)

### Recomendaciones üìù

1. **Integraci√≥n Frontend con VM rag3:**
   - ‚úÖ Documentaci√≥n completa creada (VM_RAG3_COMPLETE_ANALYSIS.md)
   - ‚è≠Ô∏è Actualizar `web/config.js` con URLs de capibara6-api
   - ‚è≠Ô∏è Configurar cliente para Milvus (b√∫squeda vectorial)
   - ‚è≠Ô∏è Configurar cliente para Nebula Graph (consultas de grafo)

2. **Scripts de Gesti√≥n:**
   - ‚è≠Ô∏è Actualizar `check-services.sh` para verificar servicios de VM rag3
   - ‚è≠Ô∏è Agregar healthchecks para Milvus (19530) y Nebula (9669)
   - ‚è≠Ô∏è Monitorear estado de workers RQ

3. **Documentaci√≥n de APIs:**
   - ‚è≠Ô∏è Documentar endpoints de capibara6-api (puerto 8000)
   - ‚è≠Ô∏è Documentar esquema de Nebula Graph
   - ‚è≠Ô∏è Documentar colecciones de Milvus

4. **Monitoreo:**
   - ‚úÖ Grafana ya configurado (puerto 3000)
   - ‚úÖ Prometheus ya configurado (puerto 9090)
   - ‚úÖ Jaeger ya configurado (puerto 16686)
   - ‚è≠Ô∏è Verificar alertas configuradas

5. **Seguridad:**
   - ‚è≠Ô∏è Verificar que puertos 19530 y 9669 no sean p√∫blicos
   - ‚è≠Ô∏è Configurar autenticaci√≥n en Milvus y Nebula
   - ‚è≠Ô∏è Revisar credenciales de PostgreSQL/Redis

---

## üìÅ Archivos de Referencia

### Documentaci√≥n Principal
- **`VM_RAG3_COMPLETE_ANALYSIS.md`** ‚≠ê NUEVO - An√°lisis completo de VM rag3
- `INFRASTRUCTURE_FINDINGS.md` (este archivo) - Hallazgos completos de infraestructura
- `BACKEND_CONSOLIDATION_PLAN.md` - Plan de consolidaci√≥n (Fases 1-4 completadas)

### Configuraci√≥n
- `web/config.js` - Configuraci√≥n completa de servicios frontend
- `backend/config/infrastructure_config.py` - Configuraci√≥n RAG y vector store
- `docker-compose.yml` - Bases de datos locales

### Scripts de Gesti√≥n (backend/)
- `start-all-services.sh` - Iniciar servicios principales
- `start-optional-services.sh` - Iniciar servicios opcionales
- `stop-all-services.sh` - Detener todos los servicios
- `check-services.sh` - Verificar estado de servicios
- `SCRIPTS_README.md` - Documentaci√≥n completa de scripts

### Herramientas de Diagn√≥stico
- `vm_rag3_diagnostic.sh` - Script de diagn√≥stico automatizado
- `VM_RAG3_INSTRUCTIONS.md` - Instrucciones para ejecutar diagn√≥stico
- `QUICK_VM_RAG3_CHECK.md` - Verificaci√≥n r√°pida

### Otros
- `SERVICES_SETUP.md` - Setup de servicios en VMs
- `ARCHITECTURE_QUICK_REF.md` - Referencia r√°pida de arquitectura
- `FIXES_ENDPOINTS.md` - Correcciones de endpoints

---

## üöÄ Estado Actual y Pr√≥ximos Pasos

### ‚úÖ Completado (2025-11-13)
1. ‚úÖ **VM rag3 documentada** - An√°lisis completo realizado
2. ‚úÖ **Milvus encontrado** - Puerto 19530, versi√≥n v2.3.10
3. ‚úÖ **Nebula Graph encontrado** - Puerto 9669, cluster de 3 nodos
4. ‚úÖ **Bridge identificado** - capibara6-api en puerto 8000
5. ‚úÖ **Fase 4 implementada** - Scripts de gesti√≥n creados y documentados
6. ‚úÖ **Toda la infraestructura mapeada** - 3 VMs con todos sus servicios

### ‚è≠Ô∏è Pr√≥ximos Pasos Recomendados
1. **Integrar frontend con VM rag3**
   - Actualizar `web/config.js` con capibara6-api endpoints
   - Configurar conexi√≥n a Milvus para b√∫squeda vectorial
   - Configurar conexi√≥n a Nebula Graph para consultas

2. **Mejorar scripts de gesti√≥n**
   - Agregar verificaci√≥n de servicios remotos en check-services.sh
   - Crear scripts de conexi√≥n a Milvus y Nebula

3. **Documentaci√≥n de APIs**
   - Documentar endpoints completos de capibara6-api
   - Crear gu√≠as de uso para Milvus y Nebula Graph

4. **Testing**
   - Probar integraci√≥n completa frontend ‚Üí bridge ‚Üí databases
   - Verificar rendimiento del sistema RAG completo
