{
  "experts": [
    {
      "expert_id": "phi4_fast",
      "model_path": "/home/elect/models/phi-4-mini",
      "domain": "general",
      "description": "Modelo r\u00e1pido para respuestas simples y directas - Optimizado para ARM Axion",
      "priority": 5,
      "quantization": null,
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.3,
      "max_num_seqs": 16,
      "enable_neon": true,
      "enable_chunked_prefill": true,
      "max_num_batched_tokens": 8192,
      "max_model_len": 2048,
      "enforce_eager": false,
      "kv_cache_dtype": "fp8",
      "use_v2_block_manager": true,
      "enable_prefix_caching": true,
      "device": "cpu",
      "dtype": "float16",
      "use_captured_graph": true,
      "swap_space": 2,
      "cpu_offload_gb": 0,
      "neon_optimizations": {
        "matmul_8x8": true,
        "flash_attention": false,
        "rmsnorm": true,
        "rope": true,
        "swiglu": true,
        "softmax_fast_exp": true
      },
      "use_cases": [
        "preguntas simples",
        "respuestas r\u00e1pidas",
        "chistes",
        "saludos",
        "respuestas directas"
      ],
      "block_size": 8,
      "num_gpu_blocks_override": 2048,
      "max_context_len_to_capture": 2048,
      "num_scheduler_steps": 2
    },
    {
      "expert_id": "mistral_balanced",
      "model_path": "/home/elect/models/mistral-7b-instruct-v0.2",
      "domain": "technical",
      "description": "Modelo equilibrado para tareas t\u00e9cnicas intermedias - Optimizado para ARM Axion",
      "priority": 4,
      "quantization": null,
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.5,
      "max_num_seqs": 16,
      "enable_neon": true,
      "enable_chunked_prefill": true,
      "max_num_batched_tokens": 8192,
      "max_model_len": 4096,
      "enforce_eager": false,
      "kv_cache_dtype": "fp8",
      "use_v2_block_manager": true,
      "enable_prefix_caching": true,
      "device": "cpu",
      "dtype": "float16",
      "use_captured_graph": true,
      "swap_space": 3,
      "cpu_offload_gb": 0,
      "neon_optimizations": {
        "matmul_8x8": true,
        "flash_attention": false,
        "rmsnorm": true,
        "rope": true,
        "swiglu": true,
        "softmax_fast_exp": true
      },
      "use_cases": [
        "explicaciones t\u00e9cnicas",
        "c\u00f3digo y programaci\u00f3n",
        "an\u00e1lisis intermedio",
        "redacci\u00f3n",
        "documentaci\u00f3n"
      ],
      "block_size": 8,
      "num_gpu_blocks_override": 2048,
      "max_context_len_to_capture": 4096,
      "num_scheduler_steps": 2
    },
    {
      "expert_id": "qwen_coder",
      "model_path": "/home/elect/models/qwen2.5-coder-1.5b",
      "domain": "coding",
      "description": "Modelo especializado en c\u00f3digo y programaci\u00f3n - Optimizado para ARM Axion",
      "priority": 3,
      "quantization": null,
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.3,
      "max_num_seqs": 16,
      "enable_neon": true,
      "enable_chunked_prefill": true,
      "max_num_batched_tokens": 8192,
      "max_model_len": 2048,
      "enforce_eager": false,
      "kv_cache_dtype": "fp8",
      "use_v2_block_manager": true,
      "enable_prefix_caching": true,
      "device": "cpu",
      "dtype": "float16",
      "use_captured_graph": true,
      "swap_space": 2,
      "cpu_offload_gb": 0,
      "neon_optimizations": {
        "matmul_8x8": true,
        "flash_attention": false,
        "rmsnorm": true,
        "rope": true,
        "swiglu": true,
        "softmax_fast_exp": true
      },
      "use_cases": [
        "generaci\u00f3n de c\u00f3digo",
        "debugging",
        "explicaciones t\u00e9cnicas",
        "refactoring",
        "documentaci\u00f3n de c\u00f3digo"
      ],
      "block_size": 8,
      "num_gpu_blocks_override": 2048,
      "max_context_len_to_capture": 2048,
      "num_scheduler_steps": 2
    },
    {
      "expert_id": "gemma3_multimodal",
      "model_path": "/home/elect/models/gemma-3-27b-it",
      "domain": "multimodal_expert",
      "description": "Modelo multimodal para texto + im\u00e1genes, an\u00e1lisis complejo y contexto largo - Optimizado para ARM Axion",
      "priority": 2,
      "quantization": null,
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.4,
      "max_num_seqs": 16,
      "enable_neon": true,
      "enable_chunked_prefill": true,
      "enable_flash_attention": true,
      "max_num_batched_tokens": 4096,
      "max_model_len": 8192,
      "trust_remote_code": true,
      "dtype": "float16",
      "enforce_eager": false,
      "kv_cache_dtype": "fp8",
      "use_v2_block_manager": true,
      "enable_prefix_caching": true,
      "device": "cpu",
      "use_captured_graph": true,
      "swap_space": 5,
      "cpu_offload_gb": 0,
      "neon_optimizations": {
        "rmsnorm": true,
        "rope": true,
        "swiglu": true,
        "flash_attention": true,
        "matmul_8x8": true,
        "softmax_fast_exp": true,
        "quantization_q4": true,
        "acl_gemm": true
      },
      "flash_attention_config": {
        "block_size": 64,
        "enable_for_seq_len_above": 512,
        "max_seq_len": 16384
      },
      "use_cases": [
        "an\u00e1lisis profundo",
        "razonamiento complejo",
        "an\u00e1lisis multimodal",
        "im\u00e1genes + texto",
        "contexto largo",
        "multiling\u00fce avanzado (140+ idiomas)",
        "research y documentaci\u00f3n extensa",
        "an\u00e1lisis de PDFs con im\u00e1genes"
      ],
      "num_gpu_blocks_override": 1024,
      "block_size": 8,
      "max_context_len_to_capture": 4096,
      "num_scheduler_steps": 2
    },
    {
      "expert_id": "aya_expanse_multilingual",
      "model_path": "/home/elect/models/aya-expanse-8b",
      "domain": "multilingual_expert",
      "description": "Modelo experto multiling\u00fce de Cohere, especializado en 23 idiomas y razonamiento complejo - Optimizado para ARM Axion",
      "priority": 2,
      "quantization": null,
      "tensor_parallel_size": 1,
      "gpu_memory_utilization": 0.4,
      "max_num_seqs": 16,
      "enable_neon": true,
      "enable_chunked_prefill": true,
      "enable_flash_attention": true,
      "max_num_batched_tokens": 4096,
      "max_model_len": 4096,
      "trust_remote_code": false,
      "dtype": "float16",
      "enforce_eager": false,
      "kv_cache_dtype": "fp8",
      "use_v2_block_manager": true,
      "enable_prefix_caching": true,
      "device": "cpu",
      "use_captured_graph": true,
      "swap_space": 4,
      "cpu_offload_gb": 0,
      "neon_optimizations": {
        "rmsnorm": true,
        "rope": true,
        "swiglu": true,
        "flash_attention": true,
        "matmul_8x8": true,
        "softmax_fast_exp": true,
        "quantization_q4": true,
        "acl_gemm": true
      },
      "flash_attention_config": {
        "block_size": 64,
        "enable_for_seq_len_above": 512,
        "max_seq_len": 4096
      },
      "use_cases": [
        "an\u00e1lisis profundo",
        "razonamiento complejo",
        "multiling\u00fce (23 idiomas)",
        "contexto largo",
        "research y documentaci\u00f3n",
        "traducci\u00f3n y localizaci\u00f3n",
        "an\u00e1lisis de idiomas no ingleses"
      ],
      "num_gpu_blocks_override": 1536,
      "block_size": 8,
      "max_context_len_to_capture": 4096,
      "num_scheduler_steps": 2
    }
  ],
  "lazy_loading": {
    "enabled": true,
    "warmup_pool_size": 1,
    "max_loaded_experts": 5,
    "memory_threshold": 0.8,
    "auto_unload_after_s": 120
  },
  "embedding_model": {
    "model_name": "all-MiniLM-L6-v2",
    "cache_size": 10000,
    "use_neon": true
  },
  "rag": {
    "enabled": false,
    "bridge_url": "http://localhost:8001",
    "collection": "capibara_docs",
    "detection_threshold": 0.5,
    "max_context_tokens": 1000,
    "notes": "Parallel RAG fetching: -40% latency on RAG queries"
  },
  "speculative_routing": {
    "enabled": false,
    "speculation_threshold": 0.85,
    "max_speculation_time": 0.5,
    "notes": "Start generation on high-confidence first chunk: -20-30% TTFT on obvious queries"
  },
  "enable_consensus": false,
  "consensus_model": "/home/elect/models/phi-4-mini",
  "chunk_size": 32,
  "routing_threshold": 0.7,
  "use_fast_classifier": true,
  "server_config": {
    "host": "0.0.0.0",
    "port": 8082,
    "log_level": "info",
    "allow_credentials": true
  },
  "performance_tuning": {
    "swap_space": 4,
    "cpu_offload_gb": 0,
    "enforce_eager": false,
    "max_context_len_to_capture": 2048,
    "enable_prefix_caching": true,
    "use_v2_block_manager": false,
    "num_scheduler_steps": 2,
    "enable_chunked_prefill": true,
    "max_num_batched_tokens": 2048,
    "use_captured_graph": true,
    "max_num_batched_tokens": 2048
  },
  "optimization_flags": {
    "arm_neon": true,
    "acl_integrated": true,
    "matmul_optimized": true,
    "memory_efficient_sampling": true,
    "batch_scheduler": "vllm_default"
  },
  "notes": {
    "deployment": "Configuraci\u00f3n completa para pruebas de todos los modelos",
    "vm_name": "models-europe",
    "memory_estimation": {
      "phi4_standard": "~2.0 GB",
      "mistral_standard": "~5.2 GB",
      "qwen_standard": "~3.2 GB",
      "gemma3_standard": "~14.0 GB",
      "aya_expanse_standard": "~5.2 GB"
    },
    "expected_performance": {
      "phi4_ttft": "~0.15s",
      "mistral_ttft": "~0.3s",
      "qwen_ttft": "~0.4s",
      "gemma3_ttft": "~0.6s",
      "aya_expanse_ttft": "~0.5s",
      "throughput_combined": "Con lazy loading: 80-100 req/min"
    }
  }
}