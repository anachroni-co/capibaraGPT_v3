#!/usr/bin/env python3
"""
Prueba segura de latencia para el sistema de consenso ARM-Axion
Con monitoreo de RAM para evitar bloqueos del servidor
"""

import requests
import time
import json
import psutil
import os
from typing import Dict, List, Optional
import statistics

def get_ram_usage_percent():
    """Obtiene el porcentaje de uso de RAM"""
    return psutil.virtual_memory().percent

def check_ram_usage():
    """Verifica si el uso de RAM excede el 90%"""
    ram_percent = get_ram_usage_percent()
    if ram_percent > 90.0:
        print(f"âš ï¸  RAM uso: {ram_percent:.1f}% - SUPERIOR AL LÃMITE DE 90%")
        print("   Terminando prueba para evitar bloqueo del servidor")
        return True
    else:
        print(f"ğŸ“Š RAM uso: {ram_percent:.1f}% - Seguro para continuar")
        return False

def is_server_responding(url: str) -> bool:
    """Verifica si el servidor estÃ¡ respondiendo"""
    try:
        response = requests.get(f"{url}/health", timeout=10)
        return response.status_code == 200
    except:
        return False

def test_consensus_latency(
    server_url: str = "http://localhost:8084",
    max_requests: int = 20,
    delay_between_requests: float = 2.0
):
    """
    Prueba de latencia segura para el sistema de consenso
    """
    print("ğŸš€ Iniciando prueba de latencia para sistema de consenso...")
    print(f"   Servidor: {server_url}")
    print(f"   MÃ¡ximo de solicitudes: {max_requests}")
    print(f"   Retraso entre solicitudes: {delay_between_requests}s")
    print("   Monitoreando uso de RAM para evitar bloqueos")
    print("="*60)

    if not is_server_responding(server_url):
        print(f"âŒ Servidor no responde en {server_url}")
        return

    # Test prompts para diferentes dominios
    test_prompts = [
        "Explica cÃ³mo se implementan las optimizaciones ARM-Axion para mejorar el rendimiento.",
        "Â¿CuÃ¡l es la diferencia entre atenciÃ³n estÃ¡ndar y Flash Attention en ARM?",
        "CÃ³mo se optimiza la memoria en un sistema con 5 modelos expertos.",
        "CÃ³mo funciona el sistema de consenso entre diferentes modelos expertos.",
        "QuÃ© ventajas tienen los kernels NEON para la inferencia en ARM Axion."
    ]

    latencies = []
    tokens_per_second_values = []
    successful_requests = 0
    failed_requests = 0

    for i in range(max_requests):
        print(f"\\n--- Solicitud {i+1}/{max_requests} ---")
        
        # Verificar uso de RAM antes de cada solicitud
        if check_ram_usage():
            print(f"âš ï¸  Prueba detenida temprano por uso elevado de RAM")
            break
            
        # Verificar si el servidor sigue respondiendo
        if not is_server_responding(server_url):
            print(f"âŒ Servidor dejÃ³ de responder en solicitud {i+1}")
            failed_requests += 1
            break

        prompt = test_prompts[i % len(test_prompts)]
        print(f"ğŸ“ Prompt: '{prompt[:50]}...'")
        
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{server_url}/v1/chat/completions",
                json={
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 100,
                    "temperature": 0.7
                },
                timeout=60  # Tiempo de espera mÃ¡s largo para consenso
            )
            
            total_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                tokens_generated = result.get('usage', {}).get('completion_tokens', 0)
                tokens_per_second = tokens_generated / total_time if total_time > 0 else 0
                
                latencies.append(total_time)
                tokens_per_second_values.append(tokens_per_second)
                successful_requests += 1
                
                print(f"âœ… Ã‰xito: {total_time:.2f}s ({tokens_per_second:.2f} tok/s)")
                
                # Mostrar modelo que respondiÃ³
                model = result.get('model', 'unknown')
                if model != 'consensus':
                    print(f"   Modelo: {model}")
                else:
                    print(f"   Consenso de mÃºltiples expertos")
            else:
                print(f"âŒ HTTP {response.status_code}: {response.text[:100]}")
                failed_requests += 1
                
        except requests.exceptions.Timeout:
            print("â° Timeout")
            failed_requests += 1
        except requests.exceptions.RequestException as e:
            print(f"ğŸ’¥ Error de red: {e}")
            failed_requests += 1
        except Exception as e:
            print(f"ğŸ’¥ Error: {e}")
            failed_requests += 1

        # Verificar RAM despuÃ©s de la solicitud
        if check_ram_usage():
            print(f"âš ï¸  Prueba detenida por uso elevado de RAM despuÃ©s de solicitud {i+1}")
            break

        # Retraso entre solicitudes
        if i < max_requests - 1:  # No esperar despuÃ©s de la Ãºltima solicitud
            print(f"â³ Esperando {delay_between_requests}s antes de siguiente solicitud...")
            time.sleep(delay_between_requests)

    # Resultados finales
    print("\\n" + "="*60)
    print("ğŸ“Š RESULTADOS FINALES DE PRUEBA DE CONSENSO")
    print("="*60)
    
    if latencies:
        print(f"âœ… Solicitudes exitosas: {successful_requests}")
        print(f"âŒ Solicitudes fallidas: {failed_requests}")
        print(f"ğŸ“Š Total de solicitudes intentadas: {successful_requests + failed_requests}")
        
        print(f"\\nâ±ï¸  RENDIMIENTO:")
        print(f"   Promedio latencia: {statistics.mean(latencies):.2f}s")
        print(f"   MÃ­nimo latencia: {min(latencies):.2f}s")
        print(f"   MÃ¡ximo latencia: {max(latencies):.2f}s")
        if len(latencies) > 1:
            print(f"   DesviaciÃ³n estÃ¡ndar: {statistics.stdev(latencies):.2f}s")
        
        if tokens_per_second_values:
            print(f"\\nâš¡ VELOCIDAD:")
            print(f"   Promedio tokens/seg: {statistics.mean(tokens_per_second_values):.2f}")
            print(f"   Rango: {min(tokens_per_second_values):.2f} - {max(tokens_per_second_values):.2f}")
        
        print(f"\\nğŸ“ˆ EFICIENCIA:")
        print(f"   Tasa de Ã©xito: {(successful_requests/(successful_requests+failed_requests)*100):.1f}%")
    else:
        print("âŒ No se completaron solicitudes exitosas")
    
    print(f"\\nğŸ’¾ RAM final: {get_ram_usage_percent():.1f}%")
    print("âœ… Prueba de latencia para sistema de consenso completada")


def main():
    """FunciÃ³n principal"""
    print("ğŸ¦« Prueba Segura de Latencia - Sistema de Consenso ARM-Axion")
    print("   Monitoreo de RAM para evitar bloqueos del servidor")
    print("   LÃ­mite: 90% de uso de RAM")
    print("="*70)
    
    # Verificar que el servidor de consenso estÃ© disponible
    server_url = "http://localhost:8084"
    print(f"ğŸ” Verificando servidor de consenso en {server_url}...")
    
    if is_server_responding(server_url):
        print("âœ… Servidor de consenso detectado")
        test_consensus_latency(server_url)
    else:
        print(f"âŒ Servidor de consenso no disponible en {server_url}")
        print("   AsegÃºrate de que el servidor estÃ© corriendo en el puerto 8084")
        print("   Puedes iniciarlo con: bash start_consensus_server.sh")


if __name__ == "__main__":
    main()