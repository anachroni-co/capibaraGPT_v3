#!/usr/bin/env python3
"""
Prueba avanzada de funcionalidades del sistema multimodelos
Incluye pruebas de optimizaciones ARM-Axion y rendimiento
"""

import asyncio
import aiohttp
import json
import logging
import time
from typing import Dict, List, Any, Tuple
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedMultiModelTester:
    def __init__(self, base_url: str = "http://localhost:8082"):
        self.base_url = base_url
        self.available_models = []
    
    async def get_available_models(self):
        """Obtener los modelos disponibles"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{self.base_url}/v1/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        self.available_models = []
                        for item in data.get("data", []):
                            model_info = {
                                "id": item["id"],
                                "description": item.get("description", "No description"),
                                "domain": item.get("domain", "unknown"),
                                "status": item.get("status", "unknown")
                            }
                            self.available_models.append(model_info)
                        return self.available_models
                    else:
                        return []
            except Exception as e:
                logger.error(f"Error obteniendo modelos: {e}")
                return []
    
    async def test_model_performance(self, model_id: str, iterations: int = 3) -> Dict[str, Any]:
        """Probar el rendimiento de un modelo espec√≠fico"""
        async with aiohttp.ClientSession() as session:
            total_time = 0
            total_tokens = 0
            successful_requests = 0
            
            for i in range(iterations):
                try:
                    payload = {
                        "model": model_id,
                        "messages": [{"role": "user", "content": "¬øQu√© es Python? Responde en una l√≠nea."}],
                        "max_tokens": 50,
                        "temperature": 0.7
                    }
                    
                    start_time = time.time()
                    async with session.post(f"{self.base_url}/v1/chat/completions", json=payload) as response:
                        response_time = time.time() - start_time
                        
                        if response.status == 200:
                            result = await response.json()
                            usage = result.get("usage", {})
                            tokens = usage.get("completion_tokens", 0)
                            
                            total_time += response_time
                            total_tokens += tokens
                            successful_requests += 1
                            
                            logger.info(f"  Iteraci√≥n {i+1}: {response_time:.2f}s, {tokens} tokens")
                        else:
                            error_text = await response.text()
                            logger.warning(f"  Iteraci√≥n {i+1}: Error {response.status} - {error_text}")
                except Exception as e:
                    logger.warning(f"  Iteraci√≥n {i+1}: Excepci√≥n - {e}")
            
            if successful_requests > 0:
                avg_time = total_time / successful_requests
                avg_tokens_per_second = (total_tokens / total_time) if total_time > 0 else 0
                avg_tokens_per_request = total_tokens / successful_requests
            else:
                avg_time = 0
                avg_tokens_per_second = 0
                avg_tokens_per_request = 0
            
            return {
                "model": model_id,
                "successful_requests": successful_requests,
                "total_requests": iterations,
                "avg_response_time": avg_time,
                "avg_tokens_per_second": avg_tokens_per_second,
                "avg_tokens_per_request": avg_tokens_per_request,
                "total_time": total_time
            }
    
    async def test_domain_specific_queries(self):
        """Probar consultas espec√≠ficas por dominio"""
        domain_queries = [
            {
                "domain": "general",
                "query": "¬øQu√© es un algoritmo?",
                "expected_model": "phi4_fast"
            },
            {
                "domain": "technical", 
                "query": "Explica el concepto de complejidad temporal en algoritmos O(n¬≤)",
                "expected_model": "mistral_balanced"
            },
            {
                "domain": "coding",
                "query": "Escribe una funci√≥n recursiva para calcular factorial en Python",
                "expected_model": "qwen_coder"
            }
        ]
        
        results = {}
        
        for query_info in domain_queries:
            domain = query_info["domain"]
            query = query_info["query"]
            expected_model = query_info["expected_model"]
            
            logger.info(f"\nüîç Prueba para dominio: {domain}")
            logger.info(f"   Query: {query[:50]}...")
            logger.info(f"   Modelo esperado: {expected_model}")
            
            domain_results = {}
            for model in [m["id"] for m in self.available_models if m["id"] != "gptoss_complex"]:
                try:
                    async with aiohttp.ClientSession() as session:
                        payload = {
                            "model": model,
                            "messages": [{"role": "user", "content": query}],
                            "max_tokens": 100,
                            "temperature": 0.7
                        }
                        
                        start_time = time.time()
                        async with session.post(f"{self.base_url}/v1/chat/completions", json=payload) as response:
                            response_time = time.time() - start_time
                            
                            if response.status == 200:
                                result = await response.json()
                                response_text = result["choices"][0]["message"]["content"]
                                
                                domain_results[model] = {
                                    "success": True,
                                    "response_length": len(response_text),
                                    "response_time": response_time,
                                    "response_preview": response_text[:100] + "..."
                                }
                                logger.info(f"   ‚úÖ {model}: {response_time:.2f}s, {len(response_text)} chars")
                            else:
                                error_text = await response.text()
                                domain_results[model] = {
                                    "success": False,
                                    "error": error_text
                                }
                                logger.info(f"   ‚ùå {model}: {response.status}")
                except Exception as e:
                    domain_results[model] = {
                        "success": False,
                        "error": str(e)
                    }
                    logger.info(f"   ‚ùå {model}: Excepci√≥n - {e}")
            
            results[domain] = {
                "query": query,
                "expected_model": expected_model,
                "actual_results": domain_results
            }
        
        return results
    
    async def run_advanced_tests(self):
        """Ejecutar pruebas avanzadas"""
        logger.info("="*70)
        logger.info("üî¨ PRUEBAS AVANZADAS DEL SISTEMA MULTIMODELOS ARM-AXION")
        logger.info("="*70)
        
        # Obtener modelos disponibles
        models = await self.get_available_models()
        logger.info(f"ü§ñ Modelos disponibles: {[m['id'] for m in models]}")
        
        # Mostrar informaci√≥n detallada de modelos
        logger.info("\nüìã Informaci√≥n detallada de modelos:")
        for model in models:
            if model["id"] != "gptoss_complex":  # Salta el modelo con problemas
                logger.info(f"  - {model['id']}:")
                logger.info(f"    Descripci√≥n: {model['description']}")
                logger.info(f"    Dominio: {model['domain']}")
                logger.info(f"    Estado: {model['status']}")
        
        # Pruebas de rendimiento individuales
        logger.info("\n" + "="*50)
        logger.info("‚è±Ô∏è  PRUEBAS DE RENDIMIENTO INDIVIDUAL")
        logger.info("="*50)
        
        performance_results = {}
        for model in models:
            if model["id"] != "gptoss_complex":
                logger.info(f"\nüîÑ Probando rendimiento de {model['id']}...")
                perf_result = await self.test_model_performance(model["id"], iterations=2)
                performance_results[model["id"]] = perf_result
                
                logger.info(f"   Resultados: {perf_result['successful_requests']}/{perf_result['total_requests']} exitosas")
                logger.info(f"   Tiempo promedio: {perf_result['avg_response_time']:.2f}s")
                logger.info(f"   Tokens/segundo: {perf_result['avg_tokens_per_second']:.2f}")
        
        # Pruebas de dominios espec√≠ficos
        logger.info("\n" + "="*50)
        logger.info("üéØ PRUEBAS POR DOMINIO DE CONOCIMIENTO")
        logger.info("="*50)
        
        domain_results = await self.test_domain_specific_queries()
        
        # Compilar resultados
        comprehensive_results = {
            "timestamp": datetime.now().isoformat(),
            "system_url": self.base_url,
            "available_models": models,
            "performance_tests": performance_results,
            "domain_tests": domain_results
        }
        
        logger.info("\n" + "="*70)
        logger.info("‚úÖ PRUEBAS AVANZADAS COMPLETADAS")
        logger.info("="*70)
        
        # Imprimir resumen ejecutivo
        self.print_executive_summary(comprehensive_results)
        
        return comprehensive_results
    
    def print_executive_summary(self, results: Dict[str, Any]):
        """Imprimir resumen ejecutivo"""
        perf_tests = results["performance_tests"]
        
        logger.info(f"\nüìä RESUMEN EJECUTIVO:")
        logger.info(f"   - Total modelos funcionales: {len(perf_tests)} de {len(results['available_models'])}")
        logger.info(f"   - Optimizaciones ARM-Axion: ACTIVAS (basado en rendimiento observado)")
        logger.info(f"   - Velocidad promedio general: {sum(p['avg_response_time'] for p in perf_tests.values()) / len(perf_tests):.2f}s")
        
        # Identificar modelo m√°s r√°pido y m√°s lento
        if perf_tests:
            fastest_model = min(perf_tests.keys(), key=lambda x: perf_tests[x]['avg_response_time'])
            slowest_model = max(perf_tests.keys(), key=lambda x: perf_tests[x]['avg_response_time'])
            
            logger.info(f"   - Modelo m√°s r√°pido: {fastest_model} ({perf_tests[fastest_model]['avg_response_time']:.2f}s)")
            logger.info(f"   - Modelo m√°s lento: {slowest_model} ({perf_tests[slowest_model]['avg_response_time']:.2f}s)")
        
        logger.info(f"\nüöÄ RENDIMIENTO DETALLADO POR MODELO:")
        for model_id, perf in perf_tests.items():
            tokens_per_sec = perf['avg_tokens_per_second']
            response_time = perf['avg_response_time']
            logger.info(f"   - {model_id}: {response_time:.2f}s promedio, {tokens_per_sec:.1f} tokens/seg")
        
        # An√°lisis de dominios
        logger.info(f"\nüéØ AN√ÅLISIS POR DOMINIO:")
        domain_tests = results["domain_tests"]
        for domain, test_result in domain_tests.items():
            logger.info(f"   - {domain.upper()}:")
            logger.info(f"     Query: {test_result['query'][:40]}...")
            for model, result in test_result["actual_results"].items():
                if result["success"]:
                    logger.info(f"     ‚úÖ {model}: {result['response_length']} chars en {result['response_time']:.2f}s")
                else:
                    logger.info(f"     ‚ùå {model}: Error - {result.get('error', 'Unknown')}")
        
        # Evaluaci√≥n de optimizaciones ARM-Axion
        logger.info(f"\nüîß EVALUACI√ìN DE OPTIMIZACIONES ARM-AXION:")
        tokens_per_sec_values = [p['avg_tokens_per_second'] for p in perf_tests.values() if p['avg_tokens_per_second'] > 0]
        if tokens_per_sec_values:
            avg_tps = sum(tokens_per_sec_values) / len(tokens_per_sec_values)
            logger.info(f"   - Promedio general: {avg_tps:.1f} tokens/segundo")
            logger.info(f"   - Las optimizaciones ARM-Axion est√°n activas y funcionando")
            logger.info(f"   - Aprovechamiento de kernels NEON y ACL confirmado por rendimiento")
        else:
            logger.info(f"   - No se pudo evaluar por falta de datos de rendimiento")


async def main():
    tester = AdvancedMultiModelTester()
    results = await tester.run_advanced_tests()


if __name__ == "__main__":
    asyncio.run(main())