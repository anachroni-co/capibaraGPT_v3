#!/usr/bin/env python3
"""
Prueba LIGHT de la pregunta especÃ­fica - versiÃ³n ultra segura
"""

import requests
import time
import json
import psutil

def get_ram_usage_percent():
    """Obtiene el porcentaje de uso de RAM"""
    return psutil.virtual_memory().percent

def is_server_responding(url: str) -> bool:
    """Verifica si el servidor estÃ¡ respondiendo"""
    try:
        response = requests.get(f"{url}/health", timeout=10)
        return response.status_code == 200
    except:
        return False

def ultra_light_consensus_test():
    """
    Prueba ultra ligera para evitar problemas de RAM
    """
    print("ğŸš€ INICIANDO PRUEBA ULTRA LIGERA DE PREGUNTA ESPECÃFICA")
    print("="*70)
    print("Pregunta: Â¿Puede el ser humano ser completamente reemplazado por las nuevas IAS y")
    print("por los robots inteligentes en los prÃ³ximos 20 aÃ±os? Â¿QuÃ© probabilidades hay?")
    print("="*70)
    
    ram_before = get_ram_usage_percent()
    print(f"ğŸ“Š RAM antes de prueba: {ram_before:.1f}%")
    
    # Intentar con servidor que estÃ© disponible
    server_url = "http://localhost:8082"  # Este tiene un modelo ya cargado
    
    print(f"ğŸ” Usando servidor: {server_url}")
    
    if not is_server_responding(server_url):
        print(f"âŒ Servidor no responde: {server_url}")
        return
    
    # Pregunta muy corta y objetivo claro para respuesta corta
    question = "Â¿PodrÃ¡n las IAs reemplazar completamente a los humanos en 20 aÃ±os? Porcentaje?"
    
    print(f"\\nğŸ“ Pregunta optimizada: '{question}'")
    
    start_time = time.time()
    
    try:
        # Solicitud MUY LIGERA para evitar problemas de RAM
        response = requests.post(
            f"{server_url}/v1/chat/completions",
            json={
                "model": "aya_expanse_multilingual",  # Modelo ya cargado
                "messages": [
                    {"role": "user", "content": question}
                ],
                "max_tokens": 25,  # MUY POQUITOS tokens para seguridad RAM
                "temperature": 0.7
            },
            timeout=45  # Tiempo razonable
        )
        
        total_time = time.time() - start_time
        ram_after = get_ram_usage_percent()
        
        print(f"\\nâ±ï¸  Tiempo de respuesta: {total_time:.2f}s")
        print(f"ğŸ“Š RAM despuÃ©s de prueba: {ram_after:.1f}%")
        print(f"ğŸ“Š Cambio RAM: {ram_after - ram_before:+.1f}%")
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            tokens_generated = result['usage']['completion_tokens']
            
            print(f"\\nâœ… RESPUESTA OBTENIDA")
            print(f"ğŸ”¢ Tokens generados: {tokens_generated}")
            print(f"âš¡ Velocidad: {tokens_generated/total_time:.2f} tokens/seg")
            
            print(f"\\nğŸ“– RESPUESTA BREVE:")
            print("-" * 40)
            print(content)
            print("-" * 40)
            
            model_used = result.get('model', 'unknown')
            print(f"\\nğŸ¤– Modelo: {model_used}")
            
            # Mostrar uso de RAM final
            final_ram = get_ram_usage_percent()
            print(f"\\nğŸ“Š RAM final: {final_ram:.1f}%")
            
        else:
            print(f"âŒ Error HTTP {response.status_code}")
            print(f"   Respuesta: {response.text[:200]}")
    
    except Exception as e:
        final_ram = get_ram_usage_percent()
        print(f"\\nğŸ“Š RAM final: {final_ram:.1f}%")
        print(f"âŒ Error: {e}")

    print("\\nâœ… Prueba ultra ligera completada con seguridad RAM")


if __name__ == "__main__":
    ultra_light_consensus_test()